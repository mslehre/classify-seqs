{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example where a conventional RNN fails, due to a lack of symmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from os import path\n",
    "from keras.utils.np_utils import to_categorical   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "$m$ sequences each of length $\\ell$ and binary class $y$.  \n",
    "class $y\\sim \\text{Bernoulli}(0.5)$  \n",
    "\n",
    "$$x=(x_1,\\ldots, x_\\ell)$$\n",
    "$x_i \\sim \\text{Bernoulli}(0.5)$ (iid)\n",
    "Falls $y_i=1$, setze\n",
    "$$x[t..t+10) \\leftarrow 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = 100\n",
    "m = 5000\n",
    "siglen = 10\n",
    "s=2 # alphabet size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "# development test data\n",
    "def simulate_data(m, t, oneHot=True, standardize=False):\n",
    "    x = np.random.randint(2, size = m*ell).reshape(m, ell)\n",
    "    y = np.random.randint(2, size = m)\n",
    "    for i in range(m):\n",
    "        if y[i]:\n",
    "            x[i, t:t + siglen] = 1\n",
    "    if oneHot:\n",
    "        x = to_categorical(x, num_classes=2)\n",
    "    if standardize: # is better for SimpleRNN, oneHot or not\n",
    "        x = x - 0.5 # standardize\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = True\n",
    "fname = \"rnn-fail-data.npz\"\n",
    "\n",
    "if regenerate or not path.exists(fname):\n",
    "    # simulate new data\n",
    "    x, y = simulate_data(3, t=20)\n",
    "    np.savez(fname, x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [1 1 1] \n",
      "x (3, 100, 2) [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load(fname)\n",
    "x = npzfile['x']\n",
    "y = npzfile['y']\n",
    "print (\"y\", y, \"\\nx\", x.shape, x[0,0:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMMCell import HMMCell, HMMLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-6.4724684e-04, -2.3025274e-04],\n",
       "       [-2.2258759e-03, -8.3214045e-04],\n",
       "       [-5.3644180e-05, -5.3048134e-06]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqLayer = HMMLayer(2, 11)\n",
    "seqLayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(bidirectional = False, seqModelType = \"SimpleRNN\", RNNunits = 32):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(None,s)))\n",
    "\n",
    "    if seqModelType == \"HMM\":\n",
    "        seqLayer = HMMLayer(5, 11)\n",
    "    elif seqModelType == \"LSTM\":\n",
    "        seqLayer = layers.LSTM(RNNunits)\n",
    "    elif seqModelType == \"GRU\":\n",
    "        seqLayer = layers.GRU(RNNunits)\n",
    "    elif seqModelType == \"SimpleRNN\":\n",
    "        seqLayer = layers.SimpleRNN(RNNunits)\n",
    "    else:\n",
    "        sys.exit(\"unknown sequence model type \" + seqModelType)\n",
    "\n",
    "    if bidirectional:\n",
    "        seqLayer = layers.Bidirectional(seqLayer)\n",
    "    \n",
    "    model.add(seqLayer)\n",
    "    model.add(layers.Dense(1))\n",
    "    lr = 1e-3\n",
    "    if seqModelType == \"HMM\":\n",
    "        lr = 1e-2\n",
    "    print (f\"lr={lr}\")\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "                  loss = tf.keras.losses.BinaryCrossentropy(), metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.001\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 32)                1120      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,153\n",
      "Trainable params: 1,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "lr=0.01\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hmm_layer_1 (HMMLayer)       (None, 5)                 770       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 776\n",
      "Trainable params: 776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "transition matrices A:\n",
      " [[[0.09059 0.09216 0.08795 0.09094 0.08877 0.09417 0.08892 0.09351 0.09069 0.09453 0.08778]\n",
      "  [0.0936  0.08765 0.08763 0.09038 0.0913  0.09167 0.09343 0.08882 0.09443 0.09171 0.08937]\n",
      "  [0.09096 0.09455 0.0925  0.09393 0.09279 0.08733 0.09007 0.09219 0.08989 0.08813 0.08766]\n",
      "  [0.08925 0.09463 0.09123 0.08847 0.092   0.08947 0.08758 0.09409 0.09052 0.08931 0.09344]\n",
      "  [0.08933 0.08794 0.08976 0.09218 0.08779 0.09476 0.09053 0.09196 0.09353 0.0916  0.09064]\n",
      "  [0.08814 0.0935  0.09351 0.0895  0.09011 0.09212 0.08857 0.08835 0.08978 0.09545 0.09097]\n",
      "  [0.08809 0.09237 0.08738 0.08843 0.09202 0.09518 0.08675 0.09412 0.09554 0.0907  0.08942]\n",
      "  [0.0905  0.09277 0.08901 0.08928 0.0909  0.09104 0.0923  0.09499 0.08937 0.08988 0.08998]\n",
      "  [0.09488 0.08688 0.09029 0.09085 0.09119 0.08688 0.09427 0.09102 0.09129 0.08771 0.09473]\n",
      "  [0.09002 0.08993 0.09443 0.09116 0.09261 0.08658 0.09147 0.09035 0.08924 0.0921  0.0921 ]\n",
      "  [0.09122 0.08861 0.08852 0.09146 0.09354 0.08755 0.09119 0.09359 0.09193 0.09251 0.08987]]\n",
      "\n",
      " [[0.09274 0.09169 0.0933  0.08702 0.0882  0.09332 0.08785 0.09471 0.09221 0.08994 0.08902]\n",
      "  [0.08862 0.0904  0.08903 0.09464 0.09194 0.08904 0.08877 0.09067 0.092   0.09244 0.09245]\n",
      "  [0.09041 0.08766 0.0896  0.08895 0.09405 0.08756 0.09029 0.09072 0.09542 0.09114 0.09421]\n",
      "  [0.08937 0.09213 0.09411 0.09032 0.0924  0.09293 0.08961 0.09152 0.08839 0.09144 0.08779]\n",
      "  [0.09309 0.08679 0.09393 0.09096 0.08933 0.08986 0.09206 0.09134 0.09217 0.09359 0.08688]\n",
      "  [0.0899  0.09304 0.09255 0.09214 0.09286 0.08687 0.09318 0.08687 0.09253 0.09219 0.08787]\n",
      "  [0.09148 0.0932  0.08669 0.08848 0.0918  0.09325 0.08795 0.0909  0.09478 0.08713 0.09432]\n",
      "  [0.09066 0.09196 0.0879  0.09297 0.08927 0.08889 0.093   0.08814 0.09551 0.08991 0.09179]\n",
      "  [0.08978 0.09146 0.08825 0.09533 0.09022 0.09336 0.09188 0.08899 0.09036 0.08946 0.09091]\n",
      "  [0.0891  0.09016 0.08973 0.09212 0.09677 0.09072 0.09478 0.09151 0.08796 0.08858 0.08857]\n",
      "  [0.09082 0.08828 0.08878 0.08837 0.09512 0.09485 0.0909  0.09116 0.0929  0.09069 0.08813]]\n",
      "\n",
      " [[0.09048 0.09326 0.08833 0.09015 0.09019 0.09281 0.08943 0.08798 0.0891  0.09518 0.09308]\n",
      "  [0.09093 0.09341 0.09465 0.08905 0.09065 0.09459 0.08925 0.08745 0.08982 0.0891  0.09111]\n",
      "  [0.09283 0.09392 0.08842 0.09336 0.0943  0.08734 0.09271 0.08749 0.09347 0.09013 0.08603]\n",
      "  [0.08963 0.09099 0.09367 0.09092 0.08598 0.09348 0.09168 0.09432 0.09003 0.08675 0.09254]\n",
      "  [0.09185 0.0937  0.09133 0.09404 0.08846 0.09363 0.09178 0.08666 0.09219 0.09049 0.08586]\n",
      "  [0.09243 0.09137 0.09155 0.09288 0.08879 0.08703 0.09365 0.09335 0.09137 0.08757 0.08999]\n",
      "  [0.08848 0.09365 0.08795 0.09165 0.09179 0.08944 0.09489 0.09387 0.08666 0.09478 0.08685]\n",
      "  [0.08729 0.09008 0.09544 0.08831 0.09027 0.0958  0.08724 0.0912  0.09155 0.09378 0.08902]\n",
      "  [0.09219 0.08888 0.09016 0.09445 0.08889 0.09415 0.09165 0.09083 0.08742 0.09225 0.08912]\n",
      "  [0.09084 0.086   0.09425 0.09015 0.08857 0.08971 0.0931  0.09029 0.09495 0.08724 0.09489]\n",
      "  [0.09106 0.08749 0.09523 0.09089 0.08769 0.09207 0.09444 0.09095 0.08846 0.09342 0.08829]]\n",
      "\n",
      " [[0.08749 0.09389 0.08719 0.09512 0.09244 0.08925 0.094   0.09059 0.09311 0.08904 0.08788]\n",
      "  [0.08753 0.08809 0.09057 0.08797 0.09163 0.09394 0.09247 0.08859 0.09438 0.0959  0.08894]\n",
      "  [0.09183 0.08845 0.08961 0.08852 0.09402 0.09228 0.09107 0.09225 0.09032 0.09175 0.08991]\n",
      "  [0.09306 0.09197 0.0863  0.08825 0.09255 0.0929  0.08779 0.09172 0.08771 0.09292 0.09485]\n",
      "  [0.09006 0.08739 0.09208 0.08874 0.09272 0.09366 0.08871 0.09098 0.09322 0.09274 0.08969]\n",
      "  [0.08869 0.09275 0.08704 0.09271 0.09026 0.08999 0.09437 0.0904  0.09162 0.09201 0.09016]\n",
      "  [0.09382 0.08904 0.08808 0.09302 0.08811 0.08745 0.09289 0.0923  0.09422 0.08692 0.09415]\n",
      "  [0.08968 0.09328 0.0948  0.08979 0.08633 0.092   0.09373 0.08624 0.09197 0.09065 0.09154]\n",
      "  [0.08879 0.09153 0.09013 0.0892  0.09524 0.09187 0.08744 0.09275 0.09254 0.09223 0.08829]\n",
      "  [0.09263 0.0957  0.08973 0.08693 0.08849 0.09575 0.08726 0.09378 0.08813 0.0881  0.09349]\n",
      "  [0.08939 0.09159 0.0906  0.09277 0.0866  0.09445 0.08852 0.09154 0.09011 0.09245 0.09197]]\n",
      "\n",
      " [[0.0915  0.09156 0.09282 0.08791 0.08822 0.09414 0.08764 0.09545 0.08818 0.09376 0.08882]\n",
      "  [0.09164 0.09225 0.08921 0.08994 0.08759 0.09337 0.09389 0.09106 0.09357 0.08542 0.09207]\n",
      "  [0.09198 0.0895  0.09095 0.09415 0.09287 0.09102 0.09062 0.09063 0.08882 0.08628 0.09318]\n",
      "  [0.08767 0.09162 0.08955 0.09041 0.08939 0.08959 0.09437 0.09086 0.09008 0.09396 0.0925 ]\n",
      "  [0.09086 0.0923  0.09285 0.08818 0.09079 0.09246 0.0901  0.08755 0.09152 0.0915  0.09188]\n",
      "  [0.089   0.09033 0.0902  0.08924 0.09216 0.08698 0.08886 0.09537 0.09418 0.09545 0.08824]\n",
      "  [0.09408 0.09042 0.08814 0.08828 0.08618 0.09332 0.093   0.09278 0.09448 0.08863 0.09068]\n",
      "  [0.09334 0.09154 0.09338 0.09303 0.08565 0.08753 0.09186 0.09277 0.0921  0.09322 0.08558]\n",
      "  [0.09019 0.09408 0.09145 0.08861 0.08749 0.09455 0.0908  0.09201 0.09501 0.08774 0.08808]\n",
      "  [0.09211 0.09148 0.0916  0.09026 0.08703 0.09314 0.09187 0.08744 0.09337 0.09151 0.09019]\n",
      "  [0.08818 0.09347 0.09292 0.08925 0.08718 0.09391 0.08702 0.09276 0.08727 0.09447 0.09357]]]\n",
      "emission matrices B:\n",
      " [[[0.50057 0.49943]\n",
      "  [0.49419 0.50581]\n",
      "  [0.49164 0.50836]\n",
      "  [0.50118 0.49882]\n",
      "  [0.5     0.5    ]\n",
      "  [0.49928 0.50072]\n",
      "  [0.47872 0.52128]\n",
      "  [0.49982 0.50018]\n",
      "  [0.48346 0.51654]\n",
      "  [0.50233 0.49767]\n",
      "  [0.48522 0.51478]]\n",
      "\n",
      " [[0.48219 0.51781]\n",
      "  [0.49344 0.50656]\n",
      "  [0.49923 0.50077]\n",
      "  [0.49725 0.50275]\n",
      "  [0.49922 0.50078]\n",
      "  [0.49325 0.50675]\n",
      "  [0.49896 0.50104]\n",
      "  [0.51186 0.48814]\n",
      "  [0.49312 0.50688]\n",
      "  [0.49162 0.50838]\n",
      "  [0.49536 0.50464]]\n",
      "\n",
      " [[0.48848 0.51152]\n",
      "  [0.50368 0.49632]\n",
      "  [0.4926  0.5074 ]\n",
      "  [0.49478 0.50522]\n",
      "  [0.48567 0.51433]\n",
      "  [0.51547 0.48453]\n",
      "  [0.49605 0.50395]\n",
      "  [0.50184 0.49816]\n",
      "  [0.50515 0.49485]\n",
      "  [0.51585 0.48415]\n",
      "  [0.505   0.495  ]]\n",
      "\n",
      " [[0.48548 0.51452]\n",
      "  [0.50423 0.49577]\n",
      "  [0.49289 0.50711]\n",
      "  [0.51187 0.48813]\n",
      "  [0.50252 0.49748]\n",
      "  [0.47928 0.52072]\n",
      "  [0.49789 0.50211]\n",
      "  [0.49222 0.50778]\n",
      "  [0.51648 0.48352]\n",
      "  [0.49005 0.50995]\n",
      "  [0.50163 0.49837]]\n",
      "\n",
      " [[0.49819 0.50181]\n",
      "  [0.49674 0.50326]\n",
      "  [0.50127 0.49873]\n",
      "  [0.50105 0.49895]\n",
      "  [0.49032 0.50968]\n",
      "  [0.4825  0.5175 ]\n",
      "  [0.5027  0.4973 ]\n",
      "  [0.50382 0.49618]\n",
      "  [0.51275 0.48725]\n",
      "  [0.48497 0.51503]\n",
      "  [0.51416 0.48584]]]\n",
      "initial distributions I:\n",
      " [[0.09197 0.09099 0.09271 0.09079 0.09246 0.08834 0.09262 0.08954 0.09199 0.08997 0.08863]\n",
      " [0.09005 0.08757 0.09191 0.09581 0.0886  0.08763 0.09002 0.09225 0.09228 0.09217 0.09172]\n",
      " [0.09275 0.08734 0.08642 0.09296 0.09467 0.09181 0.08772 0.09008 0.09435 0.09138 0.09053]\n",
      " [0.092   0.09419 0.08828 0.09305 0.08718 0.09338 0.08711 0.08971 0.08991 0.0913  0.09391]\n",
      " [0.09497 0.08967 0.09066 0.0872  0.08718 0.09104 0.09211 0.09506 0.09017 0.09415 0.08778]]\n"
     ]
    }
   ],
   "source": [
    "model = get_model(bidirectional = False, seqModelType = \"SimpleRNN\", RNNunits = 32)\n",
    "model.summary()\n",
    "model = get_model(bidirectional = False, seqModelType = \"HMM\")\n",
    "model(x)\n",
    "model.summary()\n",
    "W = model.get_layer(index=0).C\n",
    "W.print_pars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad hoc fixed prediction method\n",
    "$$ \\hat{y} = 1 :\\Leftrightarrow \\exists j: x[j..j+10) = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adhoc_pred(x):\n",
    "    x = np.squeeze(x+.5).astype('int')\n",
    "    A = np.cumsum(x, axis=1)\n",
    "    B = np.zeros_like(A)\n",
    "    B[:,siglen:] = A[:,:-siglen]\n",
    "    yhat = np.any(A-B >= siglen, axis=1).astype('int')\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adhoc_acc(x_test, y_test):\n",
    "    yhat = adhoc_pred(x_test)\n",
    "    correct = (yhat == y_test).astype('int')\n",
    "    return correct.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# ts = np.array(range(ell - 15, -1, -10))\n",
    "ts = np.array(range(ell - 15, 39, -20))\n",
    "\n",
    "len(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM dir= False \t0  lr=0.01\n",
      "transition matrices A:\n",
      " [[[0.08777 0.08829 0.0929  0.09484 0.09043 0.09016 0.08868 0.09495 0.09071 0.08881 0.09244]\n",
      "  [0.08884 0.08776 0.09029 0.09153 0.0954  0.08845 0.09117 0.09226 0.09425 0.08797 0.09207]\n",
      "  [0.09277 0.09031 0.09121 0.08736 0.08704 0.09122 0.09309 0.09214 0.09245 0.0941  0.08831]\n",
      "  [0.08691 0.09233 0.09401 0.09425 0.0921  0.0916  0.08975 0.09313 0.08722 0.08611 0.09259]\n",
      "  [0.09332 0.09205 0.09335 0.09036 0.08944 0.08985 0.08908 0.08967 0.0945  0.08861 0.08978]\n",
      "  [0.09082 0.09263 0.08859 0.08668 0.08665 0.08852 0.09535 0.09109 0.09529 0.08923 0.09514]\n",
      "  [0.09301 0.09085 0.0952  0.09096 0.09276 0.09402 0.09391 0.08652 0.08647 0.08975 0.08656]\n",
      "  [0.09024 0.08794 0.09266 0.09186 0.09035 0.08954 0.09251 0.09401 0.09053 0.0929  0.08745]\n",
      "  [0.0924  0.08584 0.09132 0.09322 0.09164 0.09356 0.09318 0.08839 0.08646 0.08956 0.09444]\n",
      "  [0.08754 0.09259 0.09408 0.09028 0.09055 0.09175 0.09392 0.08953 0.08575 0.08944 0.09458]\n",
      "  [0.09271 0.08966 0.09082 0.09194 0.0936  0.09382 0.08729 0.09193 0.08858 0.089   0.09065]]\n",
      "\n",
      " [[0.08869 0.0904  0.08899 0.09604 0.08971 0.09065 0.09235 0.09048 0.09056 0.08911 0.09302]\n",
      "  [0.08925 0.09562 0.09216 0.08847 0.09081 0.09107 0.09111 0.09153 0.08835 0.09472 0.0869 ]\n",
      "  [0.08605 0.09171 0.08872 0.09055 0.09338 0.09373 0.09248 0.09054 0.09176 0.09126 0.08982]\n",
      "  [0.08936 0.09487 0.09404 0.09293 0.09006 0.09104 0.08709 0.09247 0.09188 0.08802 0.08823]\n",
      "  [0.09004 0.09025 0.09495 0.08912 0.08911 0.08993 0.0936  0.08781 0.08834 0.09425 0.09258]\n",
      "  [0.09296 0.0893  0.0934  0.09348 0.08789 0.09191 0.09086 0.09106 0.09025 0.0911  0.0878 ]\n",
      "  [0.08796 0.08915 0.0889  0.09016 0.09244 0.09237 0.08866 0.09141 0.09194 0.09164 0.09536]\n",
      "  [0.08687 0.09377 0.09389 0.09394 0.08923 0.08897 0.08836 0.08855 0.09457 0.08897 0.09289]\n",
      "  [0.09177 0.09113 0.08995 0.09505 0.08776 0.09195 0.09603 0.08962 0.08744 0.09118 0.08811]\n",
      "  [0.09095 0.08942 0.08908 0.08959 0.09402 0.09057 0.09338 0.09002 0.08848 0.09178 0.0927 ]\n",
      "  [0.09541 0.09209 0.08743 0.09047 0.08727 0.09338 0.08771 0.09253 0.08993 0.09318 0.09061]]\n",
      "\n",
      " [[0.08825 0.09165 0.0952  0.09332 0.08914 0.09442 0.08989 0.09141 0.09132 0.08672 0.08868]\n",
      "  [0.09108 0.08854 0.09231 0.0947  0.08745 0.08931 0.0927  0.09175 0.09375 0.08759 0.09082]\n",
      "  [0.09089 0.0878  0.09248 0.09313 0.08723 0.0939  0.0937  0.0869  0.08749 0.09412 0.09236]\n",
      "  [0.08785 0.08787 0.08783 0.09504 0.08869 0.08954 0.09452 0.09492 0.09068 0.08818 0.09487]\n",
      "  [0.09081 0.09263 0.09285 0.09374 0.09087 0.08663 0.08705 0.09205 0.09377 0.08927 0.09032]\n",
      "  [0.08559 0.08629 0.0934  0.09009 0.09404 0.09379 0.09436 0.0939  0.09031 0.09043 0.0878 ]\n",
      "  [0.08863 0.09088 0.09003 0.09284 0.08719 0.09135 0.08721 0.09329 0.09412 0.09543 0.08902]\n",
      "  [0.09233 0.08957 0.08849 0.08839 0.09282 0.09074 0.0916  0.0943  0.08859 0.09386 0.08931]\n",
      "  [0.09084 0.08832 0.0937  0.08768 0.09319 0.08604 0.08607 0.09471 0.09391 0.09363 0.09191]\n",
      "  [0.09361 0.0926  0.09181 0.09129 0.09171 0.08967 0.09224 0.08765 0.09225 0.08944 0.08772]\n",
      "  [0.08971 0.09467 0.09491 0.09138 0.09029 0.08796 0.08877 0.09116 0.09312 0.09152 0.08651]]\n",
      "\n",
      " [[0.09375 0.08598 0.0948  0.09141 0.08973 0.09305 0.08971 0.08802 0.09156 0.08897 0.09301]\n",
      "  [0.08828 0.093   0.09273 0.0892  0.09224 0.09241 0.09225 0.0936  0.08954 0.08769 0.08908]\n",
      "  [0.09163 0.09387 0.08698 0.08674 0.09025 0.09223 0.09449 0.08707 0.0926  0.08997 0.09416]\n",
      "  [0.09073 0.08767 0.09268 0.09541 0.08874 0.09473 0.08981 0.08903 0.08736 0.09009 0.09373]\n",
      "  [0.08946 0.09463 0.09059 0.09191 0.08782 0.09019 0.09193 0.09172 0.0922  0.08757 0.09197]\n",
      "  [0.09065 0.09135 0.09103 0.08956 0.08716 0.09111 0.09022 0.08804 0.09436 0.09353 0.09299]\n",
      "  [0.08675 0.09444 0.09292 0.09075 0.08981 0.0948  0.09128 0.08705 0.09098 0.09255 0.08866]\n",
      "  [0.09096 0.09047 0.09352 0.0941  0.09242 0.09389 0.08877 0.08688 0.08934 0.08768 0.09198]\n",
      "  [0.09304 0.09051 0.09113 0.08796 0.08654 0.09473 0.09437 0.09214 0.08933 0.08817 0.09208]\n",
      "  [0.08939 0.09147 0.09217 0.08954 0.09027 0.09102 0.09189 0.08884 0.09344 0.09385 0.08811]\n",
      "  [0.0898  0.09075 0.08737 0.09063 0.08804 0.09467 0.09019 0.09288 0.08804 0.09426 0.09338]]\n",
      "\n",
      " [[0.09063 0.0894  0.09331 0.08666 0.09142 0.08933 0.09424 0.09251 0.09438 0.09243 0.0857 ]\n",
      "  [0.09274 0.09466 0.0868  0.09202 0.09444 0.09276 0.09094 0.08757 0.09257 0.08861 0.08688]\n",
      "  [0.08945 0.09054 0.09158 0.08893 0.09519 0.08864 0.09094 0.08738 0.09467 0.09252 0.09017]\n",
      "  [0.0857  0.09047 0.0855  0.09298 0.09386 0.09377 0.09378 0.09405 0.0895  0.09246 0.08794]\n",
      "  [0.09069 0.0892  0.08753 0.09529 0.09063 0.09581 0.09042 0.091   0.0878  0.09303 0.0886 ]\n",
      "  [0.08947 0.09103 0.09241 0.09193 0.08603 0.093   0.09214 0.08947 0.08736 0.0941  0.09307]\n",
      "  [0.08836 0.08787 0.0942  0.09088 0.09328 0.09357 0.08677 0.09246 0.0927  0.08756 0.09235]\n",
      "  [0.09274 0.08965 0.0916  0.09099 0.09061 0.09586 0.09068 0.09003 0.08712 0.089   0.09172]\n",
      "  [0.08851 0.09113 0.09462 0.08977 0.09284 0.09208 0.08918 0.08955 0.09534 0.08727 0.08972]\n",
      "  [0.09305 0.09217 0.09077 0.08635 0.09294 0.09207 0.0948  0.08635 0.09203 0.09099 0.08848]\n",
      "  [0.08957 0.08846 0.09306 0.09454 0.09497 0.09207 0.08785 0.08772 0.08914 0.09106 0.09155]]]\n",
      "emission matrices B:\n",
      " [[[0.50031 0.49969]\n",
      "  [0.48517 0.51483]\n",
      "  [0.50365 0.49635]\n",
      "  [0.50832 0.49168]\n",
      "  [0.4882  0.5118 ]\n",
      "  [0.51697 0.48303]\n",
      "  [0.50184 0.49816]\n",
      "  [0.49329 0.50671]\n",
      "  [0.50705 0.49295]\n",
      "  [0.50828 0.49172]\n",
      "  [0.47809 0.52191]]\n",
      "\n",
      " [[0.514   0.486  ]\n",
      "  [0.49545 0.50455]\n",
      "  [0.4911  0.5089 ]\n",
      "  [0.49653 0.50347]\n",
      "  [0.50437 0.49563]\n",
      "  [0.50102 0.49898]\n",
      "  [0.51524 0.48476]\n",
      "  [0.50902 0.49098]\n",
      "  [0.51787 0.48213]\n",
      "  [0.5167  0.4833 ]\n",
      "  [0.51784 0.48216]]\n",
      "\n",
      " [[0.50489 0.49511]\n",
      "  [0.49957 0.50043]\n",
      "  [0.50907 0.49093]\n",
      "  [0.50484 0.49516]\n",
      "  [0.51418 0.48582]\n",
      "  [0.50509 0.49491]\n",
      "  [0.50623 0.49377]\n",
      "  [0.48092 0.51908]\n",
      "  [0.49464 0.50536]\n",
      "  [0.50318 0.49682]\n",
      "  [0.51728 0.48272]]\n",
      "\n",
      " [[0.50336 0.49664]\n",
      "  [0.49374 0.50626]\n",
      "  [0.5124  0.4876 ]\n",
      "  [0.49212 0.50788]\n",
      "  [0.49175 0.50825]\n",
      "  [0.51544 0.48456]\n",
      "  [0.4924  0.5076 ]\n",
      "  [0.5149  0.4851 ]\n",
      "  [0.49141 0.50859]\n",
      "  [0.49057 0.50943]\n",
      "  [0.49515 0.50485]]\n",
      "\n",
      " [[0.50572 0.49428]\n",
      "  [0.49768 0.50232]\n",
      "  [0.5056  0.4944 ]\n",
      "  [0.48381 0.51619]\n",
      "  [0.49899 0.50101]\n",
      "  [0.50184 0.49816]\n",
      "  [0.50098 0.49902]\n",
      "  [0.49521 0.50479]\n",
      "  [0.49137 0.50863]\n",
      "  [0.50539 0.49461]\n",
      "  [0.48216 0.51784]]]\n",
      "initial distributions I:\n",
      " [[0.0881  0.09499 0.09379 0.08812 0.09261 0.08666 0.09134 0.08927 0.09172 0.08988 0.09351]\n",
      " [0.08705 0.09191 0.09112 0.09251 0.09241 0.09032 0.08853 0.08842 0.09115 0.09154 0.09505]\n",
      " [0.09109 0.09483 0.0938  0.08747 0.09023 0.09002 0.09097 0.09323 0.08786 0.09013 0.09036]\n",
      " [0.09073 0.09438 0.0904  0.0938  0.0873  0.08709 0.09316 0.08848 0.08932 0.09059 0.09474]\n",
      " [0.09224 0.09069 0.08746 0.09309 0.09463 0.08825 0.09391 0.08784 0.09019 0.09147 0.09023]]\n",
      "Epoch 1/40\n",
      "313/313 [==============================] - 12s 34ms/step - loss: 1.1814 - accuracy: 0.5314 - val_loss: 0.5792 - val_accuracy: 0.6800\n",
      "Epoch 2/40\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.6017 - accuracy: 0.7058 - val_loss: 0.5634 - val_accuracy: 0.6900\n",
      "Epoch 3/40\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 0.6057 - accuracy: 0.6915 - val_loss: 0.5780 - val_accuracy: 0.6300\n",
      "Epoch 4/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.6268 - accuracy: 0.7047 - val_loss: 0.5736 - val_accuracy: 0.6800\n",
      "Epoch 5/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.5851 - accuracy: 0.7077 - val_loss: 0.5069 - val_accuracy: 0.7100\n",
      "Epoch 6/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.4624 - accuracy: 0.8002 - val_loss: 0.3420 - val_accuracy: 0.8300\n",
      "Epoch 7/40\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 0.4576 - accuracy: 0.8661 - val_loss: 0.3407 - val_accuracy: 0.9100\n",
      "Epoch 8/40\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 0.3666 - accuracy: 0.8896 - val_loss: 0.2914 - val_accuracy: 0.8700\n",
      "Epoch 9/40\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.3415 - accuracy: 0.8804 - val_loss: 0.2765 - val_accuracy: 0.8800\n",
      "Epoch 10/40\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.3439 - accuracy: 0.8969 - val_loss: 0.2575 - val_accuracy: 0.9100\n",
      "Epoch 11/40\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 0.3351 - accuracy: 0.9071 - val_loss: 0.2246 - val_accuracy: 0.9100\n",
      "Epoch 12/40\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 0.3285 - accuracy: 0.9123 - val_loss: 0.2028 - val_accuracy: 0.9600\n",
      "Epoch 13/40\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 0.3020 - accuracy: 0.9253 - val_loss: 0.1817 - val_accuracy: 0.9500\n",
      "Epoch 14/40\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 0.7886 - accuracy: 0.8636 - val_loss: 0.1866 - val_accuracy: 0.9500\n",
      "Epoch 15/40\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.2485 - accuracy: 0.9389 - val_loss: 0.1733 - val_accuracy: 0.9500\n",
      "Epoch 16/40\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 0.2639 - accuracy: 0.9412 - val_loss: 0.1643 - val_accuracy: 0.9600\n",
      "Epoch 17/40\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.2724 - accuracy: 0.9406 - val_loss: 0.1598 - val_accuracy: 0.9500\n",
      "Epoch 18/40\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.3963 - accuracy: 0.8942 - val_loss: 0.1633 - val_accuracy: 0.9600\n",
      "Epoch 19/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 0.2436 - accuracy: 0.9496 - val_loss: 0.1550 - val_accuracy: 0.9600\n",
      "Epoch 20/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.2782 - accuracy: 0.9212 - val_loss: 0.1627 - val_accuracy: 0.9400\n",
      "Epoch 21/40\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.2418 - accuracy: 0.9499 - val_loss: 0.1487 - val_accuracy: 0.9600\n",
      "Epoch 22/40\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 0.2203 - accuracy: 0.9471 - val_loss: 0.1396 - val_accuracy: 0.9600\n",
      "Epoch 23/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 0.2131 - accuracy: 0.9462 - val_loss: 0.1378 - val_accuracy: 0.9500\n",
      "Epoch 24/40\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 0.3576 - accuracy: 0.9373 - val_loss: 0.1386 - val_accuracy: 0.9600\n",
      "Epoch 25/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.2710 - accuracy: 0.9410 - val_loss: 0.1445 - val_accuracy: 0.9400\n",
      "Epoch 26/40\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 0.2171 - accuracy: 0.9596 - val_loss: 0.1799 - val_accuracy: 0.9200\n",
      "Epoch 27/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.2325 - accuracy: 0.9435 - val_loss: 0.1466 - val_accuracy: 0.9400\n",
      "Epoch 28/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.2265 - accuracy: 0.9531 - val_loss: 0.1229 - val_accuracy: 0.9600\n",
      "Epoch 29/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.5633 - accuracy: 0.8264 - val_loss: 0.1589 - val_accuracy: 0.9300\n",
      "Epoch 30/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.3135 - accuracy: 0.9261 - val_loss: 0.1389 - val_accuracy: 0.9500\n",
      "Epoch 31/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.2380 - accuracy: 0.9459 - val_loss: 0.1336 - val_accuracy: 0.9500\n",
      "Epoch 32/40\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.2315 - accuracy: 0.9544 - val_loss: 0.1230 - val_accuracy: 0.9600\n",
      "Epoch 33/40\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.2257 - accuracy: 0.9542 - val_loss: 0.1272 - val_accuracy: 0.9500\n",
      "Epoch 34/40\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.2940 - accuracy: 0.9488 - val_loss: 5.8483 - val_accuracy: 0.6000\n",
      "Epoch 35/40\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 3.3418 - accuracy: 0.7048 - val_loss: 0.2883 - val_accuracy: 0.9600\n",
      "Epoch 36/40\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.3239 - accuracy: 0.9080 - val_loss: 0.2629 - val_accuracy: 0.9700\n",
      "Epoch 37/40\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.2425 - accuracy: 0.9462 - val_loss: 0.2591 - val_accuracy: 0.9700\n",
      "Epoch 38/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.2380 - accuracy: 0.9456 - val_loss: 0.1446 - val_accuracy: 0.9400\n",
      "Epoch 39/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.2481 - accuracy: 0.9460 - val_loss: 0.1484 - val_accuracy: 0.9600\n",
      "Epoch 40/40\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.2076 - accuracy: 0.9561 - val_loss: 0.1384 - val_accuracy: 0.9400\n",
      "transition matrices A:\n",
      " [[[0.84624 0.00035 0.02021 0.00964 0.00033 0.02263 0.07698 0.00034 0.01156 0.0113  0.00042]\n",
      "  [0.02561 0.03089 0.01948 0.02131 0.03301 0.03068 0.01979 0.046   0.02205 0.02124 0.72994]\n",
      "  [0.40458 0.00113 0.12727 0.02784 0.00103 0.11765 0.20226 0.00106 0.0581  0.05782 0.00125]\n",
      "  [0.37891 0.00074 0.21301 0.017   0.0007  0.16096 0.18002 0.00076 0.02382 0.02326 0.00082]\n",
      "  [0.03398 0.03356 0.02451 0.02597 0.03183 0.03848 0.02348 0.04366 0.02717 0.02629 0.69107]\n",
      "  [0.53094 0.00088 0.08022 0.01827 0.00078 0.0787  0.22828 0.0008  0.03122 0.02889 0.00102]\n",
      "  [0.47475 0.00134 0.09028 0.02965 0.0013  0.09312 0.21377 0.00113 0.04628 0.04691 0.00146]\n",
      "  [0.21956 0.00764 0.10708 0.07965 0.00819 0.16079 0.1251  0.01356 0.08778 0.09242 0.09824]\n",
      "  [0.40771 0.00088 0.17167 0.0198  0.0009  0.13451 0.20365 0.00087 0.02918 0.02973 0.00109]\n",
      "  [0.3985  0.0009  0.18145 0.0188  0.00084 0.13481 0.20597 0.00084 0.02805 0.0288  0.00103]\n",
      "  [0.00323 0.00672 0.00285 0.00334 0.00704 0.00485 0.00265 0.00333 0.00316 0.00329 0.95953]]\n",
      "\n",
      " [[0.16684 0.02461 0.02586 0.02654 0.01623 0.0212  0.17447 0.09139 0.09324 0.17468 0.18494]\n",
      "  [0.18291 0.01591 0.01699 0.01466 0.00896 0.01231 0.18674 0.10188 0.07346 0.20143 0.18476]\n",
      "  [0.1757  0.01462 0.01564 0.01437 0.00885 0.01213 0.18917 0.10476 0.07574 0.19511 0.19392]\n",
      "  [0.18577 0.01628 0.01791 0.01589 0.00903 0.01266 0.18101 0.10236 0.07883 0.18978 0.19047]\n",
      "  [0.17448 0.02571 0.03041 0.02572 0.01483 0.02112 0.17977 0.08756 0.08719 0.17999 0.17321]\n",
      "  [0.18955 0.01638 0.01896 0.01709 0.00955 0.01375 0.1845  0.10044 0.07709 0.19037 0.18231]\n",
      "  [0.16474 0.02467 0.02625 0.02531 0.01703 0.022   0.1663  0.09354 0.09801 0.17664 0.18551]\n",
      "  [0.17624 0.02244 0.02425 0.02264 0.01446 0.01824 0.17937 0.08236 0.09118 0.18368 0.18515]\n",
      "  [0.18059 0.0229  0.02431 0.02414 0.01569 0.01997 0.18966 0.07928 0.08005 0.18639 0.17702]\n",
      "  [0.16972 0.02539 0.02692 0.02579 0.01786 0.02221 0.17393 0.09227 0.09445 0.17454 0.17692]\n",
      "  [0.17732 0.02657 0.02683 0.02646 0.01707 0.02334 0.16304 0.09301 0.09486 0.17791 0.17359]]\n",
      "\n",
      " [[0.94815 0.00124 0.0025  0.02487 0.00221 0.011   0.00291 0.002   0.00168 0.00127 0.00217]\n",
      "  [0.01035 0.08921 0.15729 0.01677 0.19048 0.00984 0.07601 0.05244 0.08291 0.17693 0.13778]\n",
      "  [0.01825 0.0706  0.14456 0.04811 0.11739 0.01751 0.24432 0.07016 0.07794 0.08293 0.10824]\n",
      "  [0.0272  0.00346 0.00528 0.91585 0.00592 0.01292 0.00556 0.00819 0.00542 0.0035  0.0067 ]\n",
      "  [0.01745 0.07214 0.15476 0.03975 0.12951 0.01496 0.23533 0.06454 0.07765 0.08275 0.11117]\n",
      "  [0.16096 0.01531 0.0323  0.08775 0.02904 0.54205 0.04379 0.02427 0.02073 0.01768 0.02611]\n",
      "  [0.02232 0.0704  0.13296 0.0688  0.11287 0.023   0.20554 0.09142 0.08874 0.08048 0.10346]\n",
      "  [0.0096  0.0802  0.06493 0.00674 0.29904 0.00671 0.03398 0.03211 0.06817 0.23476 0.16377]\n",
      "  [0.0105  0.08885 0.16584 0.01418 0.20054 0.00923 0.03937 0.04846 0.08295 0.20652 0.13356]\n",
      "  [0.01144 0.08343 0.14975 0.02284 0.16696 0.01066 0.16353 0.0605  0.07866 0.12733 0.1249 ]\n",
      "  [0.01709 0.07417 0.15665 0.03782 0.12899 0.01514 0.24116 0.06293 0.07738 0.08258 0.10609]]\n",
      "\n",
      " [[1.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.     ]\n",
      "  [0.08398 0.07499 0.15682 0.07079 0.07355 0.1323  0.07421 0.12315 0.07058 0.06927 0.07037]\n",
      "  [0.23056 0.07691 0.07385 0.07388 0.07763 0.08141 0.08069 0.07641 0.07974 0.07823 0.07069]\n",
      "  [0.09338 0.07854 0.13244 0.08408 0.07858 0.1162  0.08026 0.09923 0.07648 0.07894 0.08185]\n",
      "  [0.08997 0.08119 0.14441 0.07759 0.0745  0.11682 0.07873 0.10924 0.07733 0.07351 0.07671]\n",
      "  [0.23752 0.07507 0.07475 0.0758  0.07434 0.07866 0.07662 0.07548 0.0805  0.08031 0.07094]\n",
      "  [0.08342 0.07514 0.14024 0.07101 0.07062 0.14898 0.07245 0.12666 0.0707  0.07202 0.06877]\n",
      "  [0.23355 0.0747  0.07761 0.07989 0.07905 0.08179 0.07562 0.07516 0.07644 0.07545 0.07074]\n",
      "  [0.10008 0.08686 0.095   0.08309 0.08215 0.11163 0.0904  0.09807 0.08387 0.08282 0.08603]\n",
      "  [0.08894 0.07763 0.14494 0.07474 0.07572 0.12208 0.07784 0.11031 0.07746 0.07783 0.07251]\n",
      "  [0.09392 0.08105 0.11758 0.07984 0.07796 0.12105 0.08049 0.10651 0.07711 0.08284 0.08165]]\n",
      "\n",
      " [[0.0138  0.89519 0.01389 0.00509 0.01971 0.01299 0.01388 0.0021  0.00387 0.01427 0.00522]\n",
      "  [0.13075 0.13221 0.14148 0.01218 0.09593 0.14537 0.17249 0.00472 0.00864 0.14436 0.01188]\n",
      "  [0.01478 0.88713 0.01479 0.00562 0.02142 0.01406 0.01458 0.00208 0.00413 0.0155  0.0059 ]\n",
      "  [0.00462 0.96361 0.00457 0.00263 0.00504 0.00468 0.00457 0.00094 0.00172 0.00502 0.0026 ]\n",
      "  [0.03336 0.75968 0.03129 0.01034 0.04438 0.03326 0.03228 0.00381 0.00659 0.03502 0.00997]\n",
      "  [0.01513 0.88535 0.01524 0.00583 0.02006 0.01506 0.01511 0.00213 0.00382 0.01613 0.00612]\n",
      "  [0.02026 0.8464  0.02104 0.00724 0.02911 0.02067 0.01942 0.00274 0.00508 0.02042 0.00763]\n",
      "  [0.03559 0.74399 0.03607 0.01507 0.03197 0.03666 0.03285 0.00568 0.0106  0.03593 0.01559]\n",
      "  [0.00799 0.93936 0.0085  0.00417 0.00809 0.00775 0.00727 0.00148 0.00307 0.00799 0.00433]\n",
      "  [0.01342 0.90044 0.01277 0.00487 0.01886 0.0126  0.0131  0.00188 0.00361 0.01328 0.00517]\n",
      "  [0.00476 0.96381 0.00491 0.00264 0.00505 0.00452 0.00422 0.00086 0.00168 0.00488 0.00266]]]\n",
      "emission matrices B:\n",
      " [[[0.79261 0.20739]\n",
      "  [0.0004  0.9996 ]\n",
      "  [0.82884 0.17116]\n",
      "  [0.80405 0.19595]\n",
      "  [0.0004  0.9996 ]\n",
      "  [0.81027 0.18973]\n",
      "  [0.82502 0.17498]\n",
      "  [0.00204 0.99796]\n",
      "  [0.80906 0.19094]\n",
      "  [0.80806 0.19194]\n",
      "  [0.00002 0.99998]]\n",
      "\n",
      " [[0.825   0.175  ]\n",
      "  [0.83053 0.16947]\n",
      "  [0.82909 0.17091]\n",
      "  [0.83003 0.16997]\n",
      "  [0.80908 0.19092]\n",
      "  [0.83106 0.16894]\n",
      "  [0.82497 0.17503]\n",
      "  [0.8426  0.1574 ]\n",
      "  [0.83583 0.16417]\n",
      "  [0.82358 0.17642]\n",
      "  [0.82288 0.17712]]\n",
      "\n",
      " [[0.52599 0.47401]\n",
      "  [0.19264 0.80736]\n",
      "  [0.18434 0.81566]\n",
      "  [0.00034 0.99966]\n",
      "  [0.1802  0.8198 ]\n",
      "  [0.30094 0.69906]\n",
      "  [0.18506 0.81494]\n",
      "  [0.30471 0.69529]\n",
      "  [0.1962  0.8038 ]\n",
      "  [0.17967 0.82033]\n",
      "  [0.18239 0.81761]]\n",
      "\n",
      " [[0.11882 0.88118]\n",
      "  [0.553   0.447  ]\n",
      "  [0.52039 0.47961]\n",
      "  [0.54227 0.45773]\n",
      "  [0.54269 0.45731]\n",
      "  [0.51919 0.48081]\n",
      "  [0.54815 0.45185]\n",
      "  [0.51781 0.48219]\n",
      "  [0.54102 0.45898]\n",
      "  [0.54032 0.45968]\n",
      "  [0.54843 0.45157]]\n",
      "\n",
      " [[0.89789 0.10211]\n",
      "  [0.94363 0.05637]\n",
      "  [0.89802 0.10198]\n",
      "  [0.81536 0.18464]\n",
      "  [0.90998 0.09002]\n",
      "  [0.89806 0.10194]\n",
      "  [0.90004 0.09996]\n",
      "  [0.76228 0.23772]\n",
      "  [0.78079 0.21921]\n",
      "  [0.89734 0.10266]\n",
      "  [0.81928 0.18072]]]\n",
      "initial distributions I:\n",
      " [[0.99973 0.      0.00003 0.00005 0.      0.00006 0.00003 0.      0.00004 0.00004 0.     ]\n",
      " [0.01561 0.0576  0.04629 0.06417 0.66889 0.06717 0.01678 0.0076  0.01561 0.01914 0.02113]\n",
      " [0.62146 0.04105 0.00973 0.00096 0.0205  0.00344 0.00363 0.20838 0.06301 0.01123 0.01659]\n",
      " [0.00742 0.16755 0.01777 0.12367 0.12251 0.01808 0.14475 0.01806 0.11932 0.11586 0.14503]\n",
      " [0.00874 0.01057 0.00802 0.12214 0.00628 0.00663 0.00627 0.3777  0.33743 0.00861 0.1076 ]]\n",
      "1  lr=0.01\n",
      "transition matrices A:\n",
      " [[[0.08719 0.08835 0.08925 0.08852 0.08856 0.08717 0.0942  0.09401 0.09342 0.09427 0.09506]\n",
      "  [0.09444 0.09462 0.09426 0.08712 0.09344 0.08669 0.09091 0.09436 0.08714 0.08866 0.08837]\n",
      "  [0.09348 0.08735 0.0921  0.08915 0.09103 0.09383 0.09122 0.08725 0.0919  0.09099 0.09172]\n",
      "  [0.0878  0.09455 0.0908  0.09018 0.09224 0.09234 0.08739 0.08975 0.09395 0.0882  0.09281]\n",
      "  [0.0951  0.09441 0.09139 0.08744 0.08802 0.08918 0.09004 0.09501 0.08739 0.08783 0.09419]\n",
      "  [0.09205 0.0921  0.08906 0.08789 0.09492 0.08936 0.09604 0.0884  0.08955 0.08947 0.09117]\n",
      "  [0.09187 0.0861  0.09279 0.0914  0.09443 0.09424 0.09247 0.08739 0.0912  0.08872 0.08938]\n",
      "  [0.08922 0.08796 0.09042 0.09415 0.09135 0.09451 0.0922  0.08929 0.08948 0.08797 0.09345]\n",
      "  [0.09154 0.08718 0.09388 0.0909  0.08715 0.08916 0.08846 0.08902 0.09295 0.09486 0.09491]\n",
      "  [0.09536 0.08889 0.08842 0.09023 0.08802 0.09254 0.08893 0.09579 0.0913  0.09111 0.08942]\n",
      "  [0.09298 0.09115 0.09496 0.089   0.09083 0.09164 0.09171 0.09199 0.08772 0.08981 0.0882 ]]\n",
      "\n",
      " [[0.09273 0.09403 0.08721 0.09402 0.09424 0.08697 0.09407 0.0873  0.09269 0.0861  0.09064]\n",
      "  [0.09351 0.09174 0.09119 0.09329 0.08686 0.09445 0.08817 0.09205 0.09007 0.08902 0.08965]\n",
      "  [0.09391 0.09167 0.09259 0.09064 0.09392 0.09339 0.08683 0.0885  0.08611 0.09084 0.0916 ]\n",
      "  [0.08769 0.08744 0.09578 0.09562 0.08826 0.08861 0.09033 0.09201 0.08983 0.0886  0.09583]\n",
      "  [0.09206 0.08842 0.09153 0.09277 0.08793 0.0874  0.09273 0.09239 0.09112 0.09185 0.09179]\n",
      "  [0.09541 0.08841 0.09105 0.08963 0.09068 0.09329 0.09307 0.08703 0.08983 0.09267 0.08893]\n",
      "  [0.09314 0.09454 0.08707 0.0891  0.09407 0.08607 0.09324 0.09454 0.08682 0.09004 0.09137]\n",
      "  [0.09525 0.08785 0.09103 0.09077 0.0887  0.09334 0.08748 0.08817 0.09021 0.0925  0.0947 ]\n",
      "  [0.09516 0.08773 0.08721 0.09193 0.08747 0.09362 0.08893 0.08841 0.09317 0.09213 0.09424]\n",
      "  [0.0927  0.08974 0.091   0.09122 0.08996 0.09395 0.0901  0.08715 0.09362 0.09199 0.08858]\n",
      "  [0.09044 0.09162 0.08659 0.09326 0.09338 0.0928  0.08948 0.09328 0.08746 0.08986 0.09183]]\n",
      "\n",
      " [[0.08878 0.0927  0.09203 0.08916 0.087   0.08615 0.09442 0.09442 0.09197 0.09134 0.09203]\n",
      "  [0.0889  0.09339 0.08913 0.09078 0.09355 0.08848 0.08964 0.09344 0.09246 0.09028 0.08994]\n",
      "  [0.09135 0.0912  0.09017 0.08997 0.08936 0.0905  0.09139 0.09311 0.09449 0.08962 0.08883]\n",
      "  [0.08998 0.0888  0.09013 0.08743 0.09377 0.09188 0.08893 0.09492 0.09456 0.08891 0.0907 ]\n",
      "  [0.0874  0.09352 0.08812 0.09383 0.09007 0.08943 0.09101 0.08627 0.0927  0.09406 0.09359]\n",
      "  [0.08754 0.09504 0.08817 0.08875 0.09497 0.09068 0.08855 0.09385 0.09114 0.08849 0.09281]\n",
      "  [0.09011 0.09544 0.09026 0.08926 0.0887  0.09121 0.08935 0.09043 0.09378 0.09023 0.09121]\n",
      "  [0.08736 0.09068 0.0873  0.09456 0.08808 0.09303 0.093   0.0882  0.09171 0.0907  0.09536]\n",
      "  [0.08756 0.09262 0.09349 0.08732 0.09477 0.08866 0.08816 0.09541 0.09175 0.09073 0.08953]\n",
      "  [0.09291 0.08837 0.09352 0.09085 0.09433 0.08718 0.08766 0.09114 0.09393 0.08729 0.09281]\n",
      "  [0.0902  0.08768 0.09254 0.09536 0.09154 0.09239 0.08713 0.09348 0.08799 0.0946  0.08709]]\n",
      "\n",
      " [[0.08835 0.09187 0.08735 0.09235 0.09315 0.09132 0.0942  0.08856 0.09165 0.08663 0.09456]\n",
      "  [0.09052 0.09569 0.09386 0.08702 0.09371 0.089   0.09088 0.09313 0.08812 0.08899 0.08908]\n",
      "  [0.09128 0.08637 0.09068 0.09094 0.09033 0.09401 0.09385 0.08873 0.08978 0.09341 0.09063]\n",
      "  [0.09094 0.08865 0.08955 0.08849 0.09427 0.09455 0.08683 0.08928 0.0956  0.08998 0.09186]\n",
      "  [0.09457 0.08999 0.08693 0.08951 0.09301 0.08859 0.08988 0.09357 0.09194 0.08736 0.09466]\n",
      "  [0.09435 0.09038 0.09068 0.09344 0.08739 0.09298 0.08988 0.08746 0.09275 0.08738 0.09331]\n",
      "  [0.08747 0.09508 0.08685 0.09225 0.08866 0.09492 0.08843 0.09029 0.09212 0.09132 0.09261]\n",
      "  [0.09312 0.09017 0.09281 0.09451 0.09071 0.09346 0.08585 0.08871 0.08732 0.09083 0.09251]\n",
      "  [0.09336 0.08953 0.09209 0.09284 0.09079 0.08984 0.09161 0.09496 0.09065 0.0879  0.08642]\n",
      "  [0.0929  0.08848 0.08669 0.08679 0.09384 0.09339 0.09423 0.0901  0.09192 0.08864 0.09303]\n",
      "  [0.08716 0.09019 0.0926  0.08894 0.09125 0.09149 0.09409 0.09122 0.09447 0.09032 0.08827]]\n",
      "\n",
      " [[0.09089 0.09353 0.09332 0.08694 0.09323 0.0888  0.08756 0.09061 0.09464 0.08988 0.09061]\n",
      "  [0.08966 0.09369 0.09024 0.09115 0.08724 0.09036 0.09386 0.09098 0.08811 0.09479 0.08992]\n",
      "  [0.09217 0.08933 0.08946 0.0901  0.09525 0.08705 0.08826 0.09122 0.0926  0.09471 0.08986]\n",
      "  [0.08749 0.09089 0.09475 0.09025 0.0916  0.08783 0.08932 0.09224 0.09606 0.08862 0.09095]\n",
      "  [0.09022 0.09142 0.09498 0.08918 0.08984 0.08809 0.09483 0.09175 0.08848 0.09247 0.08875]\n",
      "  [0.08813 0.09003 0.09353 0.0873  0.08844 0.09126 0.08907 0.09147 0.09328 0.09434 0.09315]\n",
      "  [0.09213 0.08897 0.09472 0.08785 0.09009 0.08944 0.08811 0.09342 0.09206 0.09091 0.09229]\n",
      "  [0.09229 0.09361 0.08978 0.09122 0.08861 0.08546 0.09121 0.09332 0.09143 0.09123 0.09183]\n",
      "  [0.09252 0.09127 0.08841 0.09047 0.09317 0.08675 0.09381 0.09001 0.08848 0.09273 0.09237]\n",
      "  [0.08908 0.09346 0.09249 0.08772 0.0899  0.09486 0.08923 0.08718 0.09127 0.09149 0.09332]\n",
      "  [0.08899 0.09188 0.08893 0.09189 0.09293 0.09222 0.09099 0.08975 0.08841 0.08963 0.09438]]]\n",
      "emission matrices B:\n",
      " [[[0.50342 0.49658]\n",
      "  [0.50238 0.49762]\n",
      "  [0.49308 0.50692]\n",
      "  [0.4851  0.5149 ]\n",
      "  [0.50139 0.49861]\n",
      "  [0.48491 0.51509]\n",
      "  [0.51215 0.48785]\n",
      "  [0.50867 0.49133]\n",
      "  [0.51349 0.48651]\n",
      "  [0.49864 0.50136]\n",
      "  [0.50488 0.49512]]\n",
      "\n",
      " [[0.52027 0.47973]\n",
      "  [0.49133 0.50867]\n",
      "  [0.50138 0.49862]\n",
      "  [0.49769 0.50231]\n",
      "  [0.50153 0.49847]\n",
      "  [0.49571 0.50429]\n",
      "  [0.48899 0.51101]\n",
      "  [0.48115 0.51885]\n",
      "  [0.49847 0.50153]\n",
      "  [0.49203 0.50797]\n",
      "  [0.50736 0.49264]]\n",
      "\n",
      " [[0.51047 0.48953]\n",
      "  [0.50359 0.49641]\n",
      "  [0.48084 0.51916]\n",
      "  [0.51393 0.48607]\n",
      "  [0.50913 0.49087]\n",
      "  [0.5087  0.4913 ]\n",
      "  [0.51766 0.48234]\n",
      "  [0.4924  0.5076 ]\n",
      "  [0.50417 0.49583]\n",
      "  [0.50569 0.49431]\n",
      "  [0.4978  0.5022 ]]\n",
      "\n",
      " [[0.50946 0.49054]\n",
      "  [0.5054  0.4946 ]\n",
      "  [0.50465 0.49535]\n",
      "  [0.50876 0.49124]\n",
      "  [0.50354 0.49646]\n",
      "  [0.51408 0.48592]\n",
      "  [0.5097  0.4903 ]\n",
      "  [0.51725 0.48275]\n",
      "  [0.48483 0.51517]\n",
      "  [0.50153 0.49847]\n",
      "  [0.49547 0.50453]]\n",
      "\n",
      " [[0.51791 0.48209]\n",
      "  [0.51485 0.48515]\n",
      "  [0.48493 0.51507]\n",
      "  [0.49154 0.50846]\n",
      "  [0.50812 0.49188]\n",
      "  [0.49208 0.50792]\n",
      "  [0.51828 0.48172]\n",
      "  [0.50964 0.49036]\n",
      "  [0.501   0.499  ]\n",
      "  [0.49208 0.50792]\n",
      "  [0.49158 0.50842]]]\n",
      "initial distributions I:\n",
      " [[0.09319 0.09296 0.09374 0.09263 0.0894  0.08748 0.09025 0.09031 0.094   0.08693 0.0891 ]\n",
      " [0.09198 0.0931  0.08935 0.08913 0.09202 0.0887  0.09278 0.08838 0.09299 0.09011 0.09148]\n",
      " [0.08904 0.08847 0.08827 0.09389 0.0929  0.08699 0.0957  0.09267 0.09012 0.08688 0.09506]\n",
      " [0.09116 0.09068 0.09033 0.09584 0.08961 0.09458 0.08989 0.08784 0.0912  0.08767 0.09119]\n",
      " [0.08735 0.08773 0.092   0.09131 0.09131 0.08834 0.08738 0.09475 0.09457 0.09329 0.09197]]\n",
      "Epoch 1/40\n",
      "313/313 [==============================] - 16s 45ms/step - loss: 8.0124 - accuracy: 0.4779 - val_loss: 0.7415 - val_accuracy: 0.4900\n",
      "Epoch 2/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.6842 - accuracy: 0.5530 - val_loss: 0.6009 - val_accuracy: 0.6700\n",
      "Epoch 3/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5944 - accuracy: 0.6824 - val_loss: 0.5879 - val_accuracy: 0.6900\n",
      "Epoch 4/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5828 - accuracy: 0.6927 - val_loss: 0.5797 - val_accuracy: 0.7000\n",
      "Epoch 5/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5787 - accuracy: 0.6970 - val_loss: 0.5856 - val_accuracy: 0.7000\n",
      "Epoch 6/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5798 - accuracy: 0.6876 - val_loss: 0.5778 - val_accuracy: 0.6900\n",
      "Epoch 7/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5847 - accuracy: 0.6799 - val_loss: 0.5823 - val_accuracy: 0.7000\n",
      "Epoch 8/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 0.5788 - accuracy: 0.6969 - val_loss: 0.6103 - val_accuracy: 0.6800\n",
      "Epoch 9/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5879 - accuracy: 0.6996 - val_loss: 0.5823 - val_accuracy: 0.6800\n",
      "Epoch 10/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5731 - accuracy: 0.7053 - val_loss: 0.5748 - val_accuracy: 0.6900\n",
      "Epoch 11/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.5664 - accuracy: 0.7049 - val_loss: 0.5440 - val_accuracy: 0.7000\n",
      "Epoch 12/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 0.5406 - accuracy: 0.7368 - val_loss: 0.4643 - val_accuracy: 0.8300\n",
      "Epoch 13/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.4555 - accuracy: 0.8218 - val_loss: 0.3737 - val_accuracy: 0.8400\n",
      "Epoch 14/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 0.3906 - accuracy: 0.8533 - val_loss: 0.3599 - val_accuracy: 0.7900\n",
      "Epoch 15/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.3435 - accuracy: 0.8796 - val_loss: 0.4462 - val_accuracy: 0.9000\n",
      "Epoch 16/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.3423 - accuracy: 0.8974 - val_loss: 0.2741 - val_accuracy: 0.8900\n",
      "Epoch 17/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 5.2681 - accuracy: 0.6266 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 18/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.7117 - accuracy: 0.5000 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 19/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 7.9445 - accuracy: 0.4850 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 20/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.9176 - accuracy: 0.4867 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 21/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 7.8124 - accuracy: 0.4935 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 22/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.8815 - accuracy: 0.4890 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 23/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 7.8451 - accuracy: 0.4914 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 24/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.9566 - accuracy: 0.4842 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 25/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 7.9370 - accuracy: 0.4854 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 26/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.9509 - accuracy: 0.4845 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 27/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 8.0278 - accuracy: 0.4796 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 28/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.9955 - accuracy: 0.4817 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 29/40\n",
      "313/313 [==============================] - 13s 41ms/step - loss: 7.7407 - accuracy: 0.4982 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 30/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.8446 - accuracy: 0.4914 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 31/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 8.1224 - accuracy: 0.4734 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 32/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.8194 - accuracy: 0.4931 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 33/40\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 7.8915 - accuracy: 0.4884 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 34/40\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 7.7925 - accuracy: 0.4948 - val_loss: 7.8667 - val_accuracy: 0.4900\n",
      "Epoch 35/40\n",
      "194/313 [=================>............] - ETA: 5s - loss: 7.9320 - accuracy: 0.4858"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-420729487425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_pars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     26\u001b[0m                           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                               verbose=1 if modelname==\"HMM\" else 0)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_reps = 1 # 30\n",
    "num_epochs = 40\n",
    "modelnames = [\"HMM\"] #\"SimpleRNN\", \"LSTM\", \"GRU\", \"ad hoc\"]\n",
    "directions = [False] # , True] # bidirectional?\n",
    "test_accs = { (name, direction) : np.empty((len(ts), num_reps)) for name in modelnames for direction in directions}\n",
    "\n",
    "for modelname in modelnames:\n",
    "    start_time = time()\n",
    "    for direction in directions:\n",
    "        print (modelname, \"dir=\", direction, \"\\t\", end = \"\")\n",
    "        for j in range(len(ts)):\n",
    "            print (j, \" \", end = \"\")\n",
    "            t = ts[j]\n",
    "            accs = test_accs[(modelname, direction)]\n",
    "            for r in range(num_reps):\n",
    "                accuracy = 0.0\n",
    "                x_test, y_test = simulate_data(1000, t)\n",
    "                if modelname == \"ad hoc\":\n",
    "                    accuracy = adhoc_acc(x_test, y_test)\n",
    "                else:\n",
    "                    model = get_model(bidirectional = direction, seqModelType = modelname, RNNunits = 32)\n",
    "                    x_train, y_train = simulate_data(m, t)\n",
    "                    x_val, y_val = simulate_data(100, t)\n",
    "                    model.get_layer(index=0).C.print_pars()\n",
    "                    model.fit(x_train, y_train,\n",
    "                          validation_data = (x_val, y_val), batch_size = 16, epochs = num_epochs,\n",
    "                              verbose=1 if modelname==\"HMM\" else 0)\n",
    "                    if modelname == \"HMM\":\n",
    "                        # print transition and emission matrices\n",
    "                        model.get_layer(index=0).C.print_pars()\n",
    "                    results = model.evaluate(x_test, y_test, batch_size = 16, verbose=0)\n",
    "                    # print (\"j=\", j, \"\\tt=\", t, \"\\tr=\", r, \"\\tresults=\", results)\n",
    "                    accuracy = results[1]\n",
    "                accs[j, r] = accuracy\n",
    "        print (\"\\n\", accs)\n",
    "    end_time = time()\n",
    "    seconds_elapsed = end_time - start_time\n",
    "    print (\"time [s]:\", seconds_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  single plot\n",
    "somemodelnames = [\"HMM\"] #[\"SimpleRNN\", \"LSTM\", \"GRU\", \"ad hoc\"]\n",
    "fig, ax = plt.subplots(1,2, figsize=(14, 5))\n",
    "for j, direction in enumerate(directions):\n",
    "    for i, model in enumerate(somemodelnames):\n",
    "        avg_test_acc = np.mean(test_accs[(model, direction)], axis = 1)\n",
    "        ax[j].plot(ts, avg_test_acc, 'o-', label = model)\n",
    "    ax[j].set_title('Performance of established ' + (\"bidirectional\" if direction else \"unidirectional\") + \" sequence models\" )\n",
    "    ax[j].set_xlabel(\"t\")\n",
    "    ax[j].set_ylabel(\"accuracy\");\n",
    "    ax[j].legend()\n",
    "print ('Accuracy was averaged over ' + str(num_reps) + ' repetitions')\n",
    "fig.savefig('failing1.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one plot each\n",
    "fig, ax = plt.subplots(len(modelnames), len(directions), squeeze=False, figsize=(25,20))\n",
    "for i, model in enumerate(modelnames):\n",
    "    for j, direction in enumerate(directions):\n",
    "        avg_test_acc = np.mean(test_accs[(model, direction)], axis = 1)\n",
    "        ax[i,j].plot(ts, avg_test_acc)\n",
    "        ax[i,j].set_ylim([.45, 1])\n",
    "        ax[i,j].set_title('Test accuracy of ' + model + ' model ' + (\"(Bidirectional)\" if direction else \"\") + ' averaged over ' + str(num_reps) + ' repetitions')\n",
    "        ax[i,j].set_xlabel(\"t\")\n",
    "        ax[i,j].set_ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('failing.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
