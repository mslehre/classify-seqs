{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather HMM example\n",
    "Das Wetter bei deinem Übersee-Chatfreund lässt sich durch eine Markowkette $X_1,X_2,\\ldots$ mit\n",
    "Zustandsraum $Q=\\{\\texttt{sun},\\texttt{rain},\\texttt{storm}\\}$ und Übergangsmatrix\n",
    "$$A=(A[r,s])_{\\scriptsize r,s \\in Q} = \\begin{pmatrix}\n",
    "0.7 & 0.2 & 0.1\\\\\n",
    "0.3 & 0.5 & 0.2\\\\\n",
    "0.2 & 0.6 & 0.2\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "beschreiben (Reihen und Spalten in der Reihenfolge `sun`, `rain`, `storm`).\n",
    "\n",
    "Dabei sei $X_i$ das Wetter am $i$-ten Tag und $X_1=\\texttt{sun}$. Dein Freund verfolgt vom Wetter abhängig Aktivitäten entweder drinnen \n",
    "(`in`) oder draußen (`out`). Sei $\\Sigma:=\\{\\texttt{in},\\texttt{out}\\}$. Folgende Matrix\n",
    "beschreibe die vom Wetter abhängenden Wahrscheinlichkeiten (Wkeiten) der Aktivitäten\n",
    "$$B=\\big(B[q,s]\\big)_{\\scriptsize\\begin{array}{l}q\\in Q\\\\s\\in \\Sigma\\end{array}} = \\begin{pmatrix}\n",
    "0.4 & 0.6 \\\\\n",
    "0.8 & 0.2 \\\\\n",
    "0.9 & 0.1 \\\\\n",
    "\\end{pmatrix}\n",
    ".$$ \n",
    "Ablesebeispiel: Dein Freund bleibt mit Wkeit 0.9 drinnen, wenn es an dem Tag stürmt (Spalten in der Reihenfolge `in`, `out`).\n",
    "Beantworte folgende Fragen für das durch $Q,\\Sigma,A,B$ und $X_1$ gegebene Hidden-Markow-Modell.\n",
    "Was ist die Wkeit \n",
    "$$P(Y_1=Y_2=Y_3=\\texttt{in}),$$\n",
    "\n",
    "dass dein Freund am allen drei Tagen drinnen bleibt?\n",
    "\n",
    "![forward DP table](forwardManually.png)\n",
    "\n",
    "**Solution: P(Y=y) = 0.1308**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # number of states\n",
    "s = 2 # emission alphabet size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_init = np.array([[7, 2, 1], [3, 5, 2], [2, 6, 2]]) / 10.0\n",
    "B_init = np.array([[4, 6], [8, 2], [9, 1]]) / 10.0\n",
    "X1_dist = np.array([1., 0., 0.]) # starts with sun\n",
    "n, s = B_init.shape # number of states, emission alphabet size\n",
    "y = np.array([0, 0, 0]) # in, in, in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions:\n",
      " [[0.7 0.2 0.1]\n",
      " [0.3 0.5 0.2]\n",
      " [0.2 0.6 0.2]] \n",
      "emissions:\n",
      " [[0.4 0.6]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"transitions:\\n\", A_init, \"\\nemissions:\\n\", B_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward recursion\n",
    "$$\\alpha[i,q] = B[q, y[i]] \\sum_{q'} \\alpha[i-1, q'] \\cdot A[q',q] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf variants of the transition and emission matrix\n",
    "A = tf.Variable(A_init, trainable = True)\n",
    "B = tf.Variable(B_init, trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Variables and Algorithm\n",
    "$$ \\alpha(q,i) = \\sum_{x_1,\\ldots, x_{i-1}\\in Q} P(x_1,\\ldots, x_{i-1}, X_i=q, y_1,\\ldots, y_i)$$\n",
    "Initialization: \n",
    "$$ \\alpha(q, 1) = \\sum_{q\\in Q} P(X_1 = q)\\cdot B[q,y[0]]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(y # observation sequence\n",
    "           ):\n",
    "    \"\"\" Forward Algorithm for Computing Sequence Likelihood \"\"\"\n",
    "    ell = y.shape[0]\n",
    "    α = tf.Variable(np.zeros([ell, n]), trainable = False)\n",
    "    \n",
    "    # initialization\n",
    "    α[0].assign(tf.multiply(B[:, y[0]], X1_dist))\n",
    "    \n",
    "    # forward algorithm\n",
    "    for i in range(1, ell):\n",
    "        # compute i-th row of DP table\n",
    "        R = tf.linalg.matvec(A, α[i-1], transpose_a = True)\n",
    "        α[i].assign(tf.multiply(B[:, y[i]], R))\n",
    "    return α\n",
    "\n",
    "def emiProb(α):\n",
    "    return np.sum(α[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4    , 0.     , 0.     ],\n",
       "       [0.112  , 0.064  , 0.036  ],\n",
       "       [0.04192, 0.0608 , 0.02808]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α = forward(y)\n",
    "α.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1308"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py = emiProb(α)\n",
    "Py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A HMM as a Special Case of a Recurrent Neural Network\n",
    "We use the notation of RNNs similar to that in [Dive into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html). $h_t$ is a size $n$ vector of RNN-\"hidden states\" (these are real numbers, not to be confused with the hidden states of HMMs, which are from $Q$).  \n",
    "$$ h_t = f(x_t, h_{t-1}; A, B)$$\n",
    "We chose the outputs\n",
    "$$ o_t = \\text{sum}(h_t) = h_t[0] + \\cdots + h_t[n-1] \\in [0,1]$$\n",
    "so that the final output $o_T$ is just the likelihood of the sequence $P(Y)$.\n",
    "This RNN does not need to produce intermediate outputs $o_t$ for $t<T$ as they are not used yet. However, they could be used in conjunction with a backwards pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMMCell\n",
    "As a template we use the code for [tf.keras.layers.SimpleRNNCell](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/recurrent.py#L1222-L1420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMMCell import HMMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the HMM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3, s=2\n"
     ]
    }
   ],
   "source": [
    "print (f\"n={n}, s={s}\")\n",
    "\n",
    "A_init = np.array([[7, 2, 1], [3, 5, 2], [2, 6, 2]]) / 10.0\n",
    "B_init = np.array([[4, 6], [8, 2], [9, 1]]) / 10.0\n",
    "I_init = np.array([1, 1e-10, 1e-10]) # start with X1=sun (very likely)\n",
    "A_init = np.log(A_init)\n",
    "B_init = np.log(B_init)\n",
    "I_init = np.log(I_init)\n",
    "\n",
    "\n",
    "A_initializer = tf.keras.initializers.Constant(A_init)\n",
    "B_initializer = tf.keras.initializers.Constant(B_init)\n",
    "I_initializer = tf.keras.initializers.Constant(I_init)\n",
    "\n",
    "yi = np.array([[1., 0]]).astype(np.float32) # np.random.random([batch_size, s]).astype(np.float32)\n",
    "states = np.array([[.4, 0, 0]]).astype(np.float32) # np.random.random([batch_size, n]).astype(np.float32)\n",
    "hmmC = HMMCell(n,\n",
    "               transition_initializer=A_initializer,\n",
    "               emission_initializer=B_initializer,\n",
    "               init_initializer=I_initializer\n",
    "              )\n",
    "\n",
    "#output = hmmC(yi, [0, states, 0.])\n",
    "#print(\"output:\\n\", output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "α=\n",
      " [[[0.4     0.      0.     ]\n",
      "  [0.112   0.064   0.036  ]\n",
      "  [0.04192 0.0608  0.02808]]] \n",
      "last column of forward table: [[0.04192 0.0608  0.02808]] \n",
      "likelihood= [0.1308]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[[1, 0],[1, 0],[1, 0]]]).astype(np.float32)\n",
    "hmm = tf.keras.layers.RNN(\n",
    "      HMMCell(n, transition_initializer=A_initializer,\n",
    "              emission_initializer=B_initializer,\n",
    "              init_initializer=I_initializer),\n",
    "      return_sequences = True,\n",
    "      return_state = True)\n",
    "  \n",
    "alpha, _, lastcol, loglik = hmm(inputs)\n",
    "alpha = alpha[1]\n",
    "\n",
    "print()\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "    print (\"α=\\n\", alpha.numpy(),\n",
    "       \"\\nlast column of forward table:\", lastcol.numpy(),\n",
    "      \"\\nlikelihood=\", loglik.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
