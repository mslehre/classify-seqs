{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather HMM example\n",
    "Das Wetter bei deinem Übersee-Chatfreund lässt sich durch eine Markowkette $X_1,X_2,\\ldots$ mit\n",
    "Zustandsraum $Q=\\{\\texttt{sun},\\texttt{rain},\\texttt{storm}\\}$ und Übergangsmatrix\n",
    "$$A=(A[r,s])_{\\scriptsize r,s \\in Q} = \\begin{pmatrix}\n",
    "0.7 & 0.2 & 0.1\\\\\n",
    "0.3 & 0.5 & 0.2\\\\\n",
    "0.2 & 0.6 & 0.2\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "beschreiben (Reihen und Spalten in der Reihenfolge `sun`, `rain`, `storm`).\n",
    "\n",
    "Dabei sei $X_i$ das Wetter am $i$-ten Tag und $X_1=\\texttt{sun}$. Dein Freund verfolgt vom Wetter abhängig Aktivitäten entweder drinnen \n",
    "(`in`) oder draußen (`out`). Sei $\\Sigma:=\\{\\texttt{in},\\texttt{out}\\}$. Folgende Matrix\n",
    "beschreibe die vom Wetter abhängenden Wahrscheinlichkeiten (Wkeiten) der Aktivitäten\n",
    "$$B=\\big(B[q,s]\\big)_{\\scriptsize\\begin{array}{l}q\\in Q\\\\s\\in \\Sigma\\end{array}} = \\begin{pmatrix}\n",
    "0.4 & 0.6 \\\\\n",
    "0.8 & 0.2 \\\\\n",
    "0.9 & 0.1 \\\\\n",
    "\\end{pmatrix}\n",
    ".$$ \n",
    "Ablesebeispiel: Dein Freund bleibt mit Wkeit 0.9 drinnen, wenn es an dem Tag stürmt (Spalten in der Reihenfolge `in`, `out`).\n",
    "Beantworte folgende Fragen für das durch $Q,\\Sigma,A,B$ und $X_1$ gegebene Hidden-Markow-Modell.\n",
    "Was ist die Wkeit \n",
    "$$P(Y_1=Y_2=Y_3=\\texttt{in}),$$\n",
    "\n",
    "dass dein Freund am allen drei Tagen drinnen bleibt?\n",
    "\n",
    "![forward DP table](forwardManually.png)\n",
    "\n",
    "**Solution: P(Y=y) = 0.1308**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # number of states\n",
    "s = 2 # emission alphabet size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_init = np.array([[7, 2, 1], [3, 5, 2], [2, 6, 2]]) / 10.0\n",
    "B_init = np.array([[4, 6], [8, 2], [9, 1]]) / 10.0\n",
    "X1_dist = np.array([1., 0., 0.]) # starts with sun\n",
    "n, s = B_init.shape # number of states, emission alphabet size\n",
    "y = np.array([0, 0, 0]) # in, in, in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions:\n",
      " [[0.7 0.2 0.1]\n",
      " [0.3 0.5 0.2]\n",
      " [0.2 0.6 0.2]] \n",
      "emissions:\n",
      " [[0.4 0.6]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"transitions:\\n\", A_init, \"\\nemissions:\\n\", B_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward recursion\n",
    "$$\\alpha[i,q] = B[q, y[i]] \\sum_{q'} \\alpha[i-1, q'] \\cdot A[q',q] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf variants of the transition and emission matrix\n",
    "A = tf.Variable(A_init, trainable = True)\n",
    "B = tf.Variable(B_init, trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Variables and Algorithm\n",
    "$$ \\alpha(q,i) = \\sum_{x_1,\\ldots, x_{i-1}\\in Q} P(x_1,\\ldots, x_{i-1}, X_i=q, y_1,\\ldots, y_i)$$\n",
    "Initialization: \n",
    "$$ \\alpha(q, 1) = \\sum_{q\\in Q} P(X_1 = q)\\cdot B[q,y[0]]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(y # observation sequence\n",
    "           ):\n",
    "    \"\"\" Forward Algorithm for Computing Sequence Likelihood \"\"\"\n",
    "    ell = y.shape[0]\n",
    "    α = tf.Variable(np.zeros([ell, n]), trainable = False)\n",
    "    \n",
    "    # initialization\n",
    "    α[0].assign(tf.multiply(B[:, y[0]], X1_dist))\n",
    "    \n",
    "    # forward algorithm\n",
    "    for i in range(1, ell):\n",
    "        # compute i-th row of DP table\n",
    "        R = tf.linalg.matvec(A, α[i-1], transpose_a = True)\n",
    "        α[i].assign(tf.multiply(B[:, y[i]], R))\n",
    "    return α\n",
    "\n",
    "def emiProb(α):\n",
    "    return np.sum(α[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4    , 0.     , 0.     ],\n",
       "       [0.112  , 0.064  , 0.036  ],\n",
       "       [0.04192, 0.0608 , 0.02808]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α = forward(y)\n",
    "α.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1308"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py = emiProb(α)\n",
    "Py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A HMM as a Special Case of a Recurrent Neural Network\n",
    "We use the notation of RNNs similar to that in [Dive into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html). $h_t$ is a size $n$ vector of RNN-\"hidden states\" (these are real numbers, not to be confused with the hidden states of HMMs, which are from $Q$).  \n",
    "$$ h_t = f(x_t, h_{t-1}; A, B)$$\n",
    "We chose the outputs\n",
    "$$ o_t = \\text{sum}(h_t) = h_t[0] + \\cdots + h_t[n-1] \\in [0,1]$$\n",
    "so that the final output $o_T$ is just the likelihood of the sequence $P(Y)$.\n",
    "This RNN does not need to produce intermediate outputs $o_t$ for $t<T$ as they are not used yet. However, they could be used in conjunction with a backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNCell\n",
    "As a template we use the code for [tf.keras.layers.SimpleRNNCell](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/recurrent.py#L1222-L1420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "class HMMCell(Layer):\n",
    "  \"\"\"Cell class for a HMM as a RNN.\n",
    "  This class processes one step within the whole time sequence input.\n",
    "  Arguments:\n",
    "    n: positive integer number of hidden states, dimensionality of the output space.\n",
    "  Call arguments:\n",
    "    inputs: A 2D tensor, with shape of `[batch, feature]`. feature=s is emission alphabet size\n",
    "    states: A 2D tensor with shape of `[batch, n]`, which is the forward variable alpha from\n",
    "      the previous time step. For timestep 0, the initial state provided by user\n",
    "      will be feed to cell.\n",
    "  Examples:\n",
    "  ```python\n",
    "  inputs = np.random.random([32, 3, 2]).astype(np.float32)\n",
    "  hmmC = HMMCell(3)\n",
    "  output = hmmC(inputs)  # The output has shape `[32, 3]`.\n",
    "  hmm = tf.keras.layers.RNN(\n",
    "      HMMCell(3),\n",
    "      return_sequences = True,\n",
    "      return_state = True)\n",
    "  # whole_sequence_output has shape `[32, 3, 3]`.\n",
    "  # final_state has shape `[32, 3]`.\n",
    "  whole_sequence_output, final_state = hmm(inputs)\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               n, # number of HMM hidden states, output size\n",
    "               **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.n = n\n",
    "    self.state_size = self.n\n",
    "    self.output_size = self.n\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.emission_kernel = self.add_weight(\n",
    "        shape=(self.n, input_shape[-1]),\n",
    "        name='emission_kernel') # closely related to B\n",
    "    self.transition_kernel = self.add_weight(\n",
    "        shape=(self.n, self.n),\n",
    "        name='transition_kernel') # closely related to A\n",
    "    self.built = True\n",
    "\n",
    "  def call(self, inputs, states, training=None):\n",
    "    prev_output = states\n",
    "    # convert parameter matrices to stochastic matrices for transition (A) and emission probs (B)\n",
    "    # TODO: this could be more efficient, maybe using tensorflow.python.keras.constraints?\n",
    "    A = tf.nn.softmax(self.transition_kernel, axis=-1, name=\"A\")\n",
    "    B = tf.nn.softmax(self.emission_kernel, axis=-1, name=\"B\")\n",
    "    print (\"A=\\n\", A)\n",
    "    print (\"prev_output=\\n\", prev_output)\n",
    "    R = tf.linalg.matvec(A, prev_output, transpose_a = True)\n",
    "    E = tf.linalg.matvec(B, inputs, transpose_a=False, name=\"E\")\n",
    "    output = tf.multiply(E, R)\n",
    "    \n",
    "    new_state = output\n",
    "    return output, new_state\n",
    "\n",
    "  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'n': self.units\n",
    "    }\n",
    "    config.update(_config_for_enable_caching_device(self))\n",
    "    base_config = super().get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3, s=2, batch_size=4\n",
      "A=\n",
      " tf.Tensor(\n",
      "[[0.09264284 0.41968825 0.48766893]\n",
      " [0.3085003  0.08489413 0.6066056 ]\n",
      " [0.48667473 0.28925818 0.22406708]], shape=(3, 3), dtype=float32)\n",
      "prev_output=\n",
      " [[0.98690563 0.82522434 0.37491477]\n",
      " [0.32643446 0.64445996 0.10785879]\n",
      " [0.35583916 0.22308438 0.4507376 ]\n",
      " [0.16685499 0.19147485 0.9160768 ]]\n",
      "output:\n",
      " tf.Tensor(\n",
      "[[0.3833822  0.42148274 0.6673132 ]\n",
      " [0.25967288 0.20550083 0.52807736]\n",
      " [0.25829396 0.23246434 0.25593612]\n",
      " [0.40555367 0.27698553 0.33952895]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "print (f\"n={n}, s={s}, batch_size={batch_size}\")\n",
    "inputs = np.random.random([batch_size, n, s]).astype(np.float32)\n",
    "yi = np.random.random([batch_size, s]).astype(np.float32)\n",
    "states = np.random.random([batch_size, n]).astype(np.float32)\n",
    "hmmC = HMMCell(n)\n",
    "\n",
    "output = hmmC(yi, states)\n",
    "print(\"output:\\n\", output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DropoutRNNCellMixin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6bdbf735f32a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSimpleRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropoutRNNCellMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"Cell class for SimpleRNN.\n\u001b[1;32m      3\u001b[0m   \u001b[0mSee\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthe\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mRNN\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0musage\u001b[0m \u001b[0mof\u001b[0m \u001b[0mRNN\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mThis\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0mone\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mwithin\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mwhole\u001b[0m \u001b[0mtime\u001b[0m \u001b[0msequence\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhereas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DropoutRNNCellMixin' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n",
    "  \"\"\"Cell class for SimpleRNN.\n",
    "  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n",
    "  for details about the usage of RNN API.\n",
    "  This class processes one step within the whole time sequence input, whereas\n",
    "  `tf.keras.layer.SimpleRNN` processes the whole sequence.\n",
    "  Arguments:\n",
    "    units: Positive integer, dimensionality of the output space.\n",
    "    activation: Activation function to use.\n",
    "      Default: hyperbolic tangent (`tanh`).\n",
    "      If you pass `None`, no activation is applied\n",
    "      (ie. \"linear\" activation: `a(x) = x`).\n",
    "    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n",
    "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "      used for the linear transformation of the inputs. Default:\n",
    "      `glorot_uniform`.\n",
    "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "      weights matrix, used for the linear transformation of the recurrent state.\n",
    "      Default: `orthogonal`.\n",
    "    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n",
    "    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
    "      matrix. Default: `None`.\n",
    "    recurrent_regularizer: Regularizer function applied to the\n",
    "      `recurrent_kernel` weights matrix. Default: `None`.\n",
    "    bias_regularizer: Regularizer function applied to the bias vector. Default:\n",
    "      `None`.\n",
    "    kernel_constraint: Constraint function applied to the `kernel` weights\n",
    "      matrix. Default: `None`.\n",
    "    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n",
    "      weights matrix. Default: `None`.\n",
    "    bias_constraint: Constraint function applied to the bias vector. Default:\n",
    "      `None`.\n",
    "    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n",
    "      transformation of the inputs. Default: 0.\n",
    "    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n",
    "      the linear transformation of the recurrent state. Default: 0.\n",
    "  Call arguments:\n",
    "    inputs: A 2D tensor, with shape of `[batch, feature]`.\n",
    "    states: A 2D tensor with shape of `[batch, units]`, which is the state from\n",
    "      the previous time step. For timestep 0, the initial state provided by user\n",
    "      will be feed to cell.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode or in inference mode. Only relevant when `dropout` or\n",
    "      `recurrent_dropout` is used.\n",
    "  Examples:\n",
    "  ```python\n",
    "  inputs = np.random.random([32, 10, 8]).astype(np.float32)\n",
    "  rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))\n",
    "  output = rnn(inputs)  # The output has shape `[32, 4]`.\n",
    "  rnn = tf.keras.layers.RNN(\n",
    "      tf.keras.layers.SimpleRNNCell(4),\n",
    "      return_sequences=True,\n",
    "      return_state=True)\n",
    "  # whole_sequence_output has shape `[32, 10, 4]`.\n",
    "  # final_state has shape `[32, 4]`.\n",
    "  whole_sequence_output, final_state = rnn(inputs)\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               units,\n",
    "               activation='tanh',\n",
    "               use_bias=True,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               recurrent_initializer='orthogonal',\n",
    "               bias_initializer='zeros',\n",
    "               kernel_regularizer=None,\n",
    "               recurrent_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               recurrent_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               **kwargs):\n",
    "    # By default use cached variable under v2 mode, see b/143699808.\n",
    "    if ops.executing_eagerly_outside_functions():\n",
    "      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n",
    "    else:\n",
    "      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "    super(SimpleRNNCell, self).__init__(**kwargs)\n",
    "    self.units = units\n",
    "    self.activation = activations.get(activation)\n",
    "    self.use_bias = use_bias\n",
    "\n",
    "    self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "    self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "    self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "    self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "    self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "    self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "    self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    self.dropout = min(1., max(0., dropout))\n",
    "    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "    self.state_size = self.units\n",
    "    self.output_size = self.units\n",
    "\n",
    "  @tf_utils.shape_type_conversion\n",
    "  def build(self, input_shape):\n",
    "    default_caching_device = _caching_device(self)\n",
    "    self.kernel = self.add_weight(\n",
    "        shape=(input_shape[-1], self.units),\n",
    "        name='kernel',\n",
    "        initializer=self.kernel_initializer,\n",
    "        regularizer=self.kernel_regularizer,\n",
    "        constraint=self.kernel_constraint,\n",
    "        caching_device=default_caching_device)\n",
    "    self.recurrent_kernel = self.add_weight(\n",
    "        shape=(self.units, self.units),\n",
    "        name='recurrent_kernel',\n",
    "        initializer=self.recurrent_initializer,\n",
    "        regularizer=self.recurrent_regularizer,\n",
    "        constraint=self.recurrent_constraint,\n",
    "        caching_device=default_caching_device)\n",
    "    if self.use_bias:\n",
    "      self.bias = self.add_weight(\n",
    "          shape=(self.units,),\n",
    "          name='bias',\n",
    "          initializer=self.bias_initializer,\n",
    "          regularizer=self.bias_regularizer,\n",
    "          constraint=self.bias_constraint,\n",
    "          caching_device=default_caching_device)\n",
    "    else:\n",
    "      self.bias = None\n",
    "    self.built = True\n",
    "\n",
    "  def call(self, inputs, states, training=None):\n",
    "    prev_output = states[0] if nest.is_nested(states) else states\n",
    "    dp_mask = self.get_dropout_mask_for_cell(inputs, training)\n",
    "    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "        prev_output, training)\n",
    "\n",
    "    if dp_mask is not None:\n",
    "      h = K.dot(inputs * dp_mask, self.kernel)\n",
    "    else:\n",
    "      h = K.dot(inputs, self.kernel)\n",
    "    if self.bias is not None:\n",
    "      h = K.bias_add(h, self.bias)\n",
    "\n",
    "    if rec_dp_mask is not None:\n",
    "      prev_output = prev_output * rec_dp_mask\n",
    "    output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "    if self.activation is not None:\n",
    "      output = self.activation(output)\n",
    "\n",
    "    new_state = [output] if nest.is_nested(states) else output\n",
    "    return output, new_state\n",
    "\n",
    "  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'units':\n",
    "            self.units,\n",
    "        'activation':\n",
    "            activations.serialize(self.activation),\n",
    "        'use_bias':\n",
    "            self.use_bias,\n",
    "        'kernel_initializer':\n",
    "            initializers.serialize(self.kernel_initializer),\n",
    "        'recurrent_initializer':\n",
    "            initializers.serialize(self.recurrent_initializer),\n",
    "        'bias_initializer':\n",
    "            initializers.serialize(self.bias_initializer),\n",
    "        'kernel_regularizer':\n",
    "            regularizers.serialize(self.kernel_regularizer),\n",
    "        'recurrent_regularizer':\n",
    "            regularizers.serialize(self.recurrent_regularizer),\n",
    "        'bias_regularizer':\n",
    "            regularizers.serialize(self.bias_regularizer),\n",
    "        'kernel_constraint':\n",
    "            constraints.serialize(self.kernel_constraint),\n",
    "        'recurrent_constraint':\n",
    "            constraints.serialize(self.recurrent_constraint),\n",
    "        'bias_constraint':\n",
    "            constraints.serialize(self.bias_constraint),\n",
    "        'dropout':\n",
    "            self.dropout,\n",
    "        'recurrent_dropout':\n",
    "            self.recurrent_dropout\n",
    "    }\n",
    "    config.update(_config_for_enable_caching_device(self))\n",
    "    base_config = super(SimpleRNNCell, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
