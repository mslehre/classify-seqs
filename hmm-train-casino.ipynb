{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A HMM as a Special Case of a Recurrent Neural Network\n",
    "We use the notation of RNNs similar to that in [Dive into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html). $h_t$ is a size $n$ vector of RNN-\"hidden states\" (these are real numbers, not to be confused with the hidden states of HMMs, which are from $Q$).  \n",
    "$$ h_t = f(x_t, h_{t-1}; A, B)$$\n",
    "We chose the outputs\n",
    "$$ o_t = \\text{sum}(h_t) = h_t[0] + \\cdots + h_t[n-1] \\in [0,1]$$\n",
    "so that the final output $o_T$ is just the likelihood of the sequence $P(Y)$.\n",
    "This RNN does not need to produce intermediate outputs $o_t$ for $t<T$ as they are not used yet. However, they could be used in conjunction with a backwards pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMMCell\n",
    "As a template we use the code for [tf.keras.layers.SimpleRNNCell](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/recurrent.py#L1222-L1420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMMCell import HMMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the HMM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " tf.Tensor([[-1.551169]], shape=(1, 1), dtype=float32) \n",
      "\n",
      "transition matrix A:\n",
      " [[[0.7 0.2 0.1]\n",
      "  [0.3 0.5 0.2]\n",
      "  [0.2 0.6 0.2]]]\n",
      "emission matrix B:\n",
      " [[[0.4 0.6]\n",
      "  [0.8 0.2]\n",
      "  [0.9 0.1]]]\n",
      "initial distribution I:\n",
      " [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "\n",
    "A_init = np.array([[[7, 2, 1], [3, 5, 2], [2, 6, 2]]]) / 10.0\n",
    "B_init = np.array([[[4, 6], [8, 2], [9, 1]]]) / 10.0\n",
    "I_init = np.array([[1, 1e-10, 1e-10]]) # start with X1=sun (very likely)\n",
    "\n",
    "# take ln to cancel out the softmax that is applied to obtain a stochastic matrix \n",
    "A_init = np.log(A_init)\n",
    "B_init = np.log(B_init)\n",
    "I_init = np.log(I_init)\n",
    "\n",
    "A_initializer = tf.keras.initializers.Constant(A_init)\n",
    "B_initializer = tf.keras.initializers.Constant(B_init)\n",
    "I_initializer = tf.keras.initializers.Constant(I_init)\n",
    "\n",
    "yi = np.array([[1., 0]]).astype(np.float32) # np.random.random([batch_size, s]).astype(np.float32)\n",
    "states = np.array([[[0.4, 0, 0]]]).astype(np.float32) # np.random.random([batch_size, n]).astype(np.float32)\n",
    "hmmC = HMMCell(units=1, n=n,\n",
    "               transition_initializer=A_initializer,\n",
    "               emission_initializer=B_initializer,\n",
    "               init_initializer=I_initializer)\n",
    "\n",
    "output = hmmC(yi, [0, states, [0.]])\n",
    "print(\"output:\\n\", output[0], \"\\n\")\n",
    "\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "    print(\"transition matrix A:\\n\", hmmC.A.numpy())\n",
    "    print(\"emission matrix B:\\n\", hmmC.B.numpy())\n",
    "    print(\"initial distribution I:\\n\", hmmC.I.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "log-likelihood= [<tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
      "array([[[-0.91629],\n",
      "        [-1.55117],\n",
      "        [-2.03409]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=int8, numpy=array([0], dtype=int8)>, <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[0.32049, 0.46483, 0.21468]]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-2.03409]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[[1, 0],[1, 0],[1, 0]]]).astype(np.float32)\n",
    "hmm = tf.keras.layers.RNN(hmmC, return_sequences = True, return_state = True)\n",
    "  \n",
    "#alpha, _, lastcol, \n",
    "loglik = hmm(inputs)\n",
    "#alpha = alpha[1]\n",
    "\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "    #print (\"Î±=\\n\", alpha.numpy(),\n",
    "    #\"\\nlast column of forward table:\", lastcol.numpy())\n",
    "    print(\"\\nlog-likelihood=\", loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 100, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dishonest_casino import get_casino_dataset\n",
    "batch_size = 32\n",
    "ds = get_casino_dataset().repeat().batch(batch_size)\n",
    "\n",
    "for inputs in ds.take(1):\n",
    "    pass\n",
    "\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "u=2\n",
    "s = inputs.shape[-1]\n",
    "dcc = HMMCell(u, n)\n",
    "\n",
    "# test HMMCell and initialize emission alphabet size\n",
    "Q = dcc(inputs[:,0,:], [tf.zeros(batch_size, dtype=tf.int8),\n",
    "                    tf.ones([batch_size, u, n], dtype=tf.float32), \n",
    "                    tf.zeros([batch_size, u], dtype=tf.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model\n",
    "F = tf.keras.layers.RNN(dcc, return_state = True)\n",
    "len(F(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pars(cell):\n",
    "    with np.printoptions(precision=5, suppress=True):\n",
    "        print(\"transition matrices A:\\n\", cell.A.numpy())\n",
    "        print(\"emission matrices B:\\n\", cell.B.numpy())\n",
    "        print(\"initial distributions I:\\n\", cell.I.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrices A:\n",
      " [[[0.50019 0.49981]\n",
      "  [0.50203 0.49797]]\n",
      "\n",
      " [[0.5049  0.4951 ]\n",
      "  [0.49818 0.50182]]]\n",
      "emission matrices B:\n",
      " [[[0.16853 0.16243 0.16842 0.16354 0.16395 0.17314]\n",
      "  [0.16437 0.17153 0.16509 0.16757 0.16828 0.16315]]\n",
      "\n",
      " [[0.17257 0.16865 0.15846 0.16822 0.16189 0.17021]\n",
      "  [0.171   0.1614  0.17441 0.16672 0.16194 0.16453]]]\n",
      "initial distributions I:\n",
      " [[0.48994 0.51006]\n",
      " [0.4927  0.5073 ]]\n"
     ]
    }
   ],
   "source": [
    "print_pars(dcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss test: 179.1325225830078\n"
     ]
    }
   ],
   "source": [
    "def loss(model, y):\n",
    "  alpha, _, lastcol, loglik = model(y)\n",
    "  L = -tf.reduce_mean(loglik)\n",
    "  return L\n",
    "\n",
    "L = loss(F, inputs)\n",
    "#print(f\"likelihoods = {lik}\\nloss (avg neg log lik)= {loss}\")\n",
    "print(\"Loss test: {}\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Initial Loss: 179.1325225830078\n"
     ]
    }
   ],
   "source": [
    "model = F\n",
    "loss_value, grads = grad(model, inputs)\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(opt.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 178.562\n",
      "transition matrices A:\n",
      " [[[0.54892 0.45108]\n",
      "  [0.55017 0.44983]]\n",
      "\n",
      " [[0.55359 0.44641]\n",
      "  [0.54661 0.45339]]]\n",
      "emission matrices B:\n",
      " [[[0.16306 0.15654 0.16213 0.15843 0.1577  0.20215]\n",
      "  [0.15948 0.16573 0.15928 0.16281 0.16218 0.19053]]\n",
      "\n",
      " [[0.16633 0.16278 0.15274 0.1627  0.15661 0.19884]\n",
      "  [0.16504 0.15607 0.16832 0.16155 0.15692 0.1921 ]]]\n",
      "initial distributions I:\n",
      " [[0.46595 0.53405]\n",
      " [0.48945 0.51055]]\n",
      "Epoch 002: Loss: 177.140\n",
      "Epoch 004: Loss: 176.980\n",
      "Epoch 006: Loss: 177.186\n",
      "Epoch 008: Loss: 177.049\n",
      "Epoch 010: Loss: 177.308\n",
      "transition matrices A:\n",
      " [[[0.63618 0.36382]\n",
      "  [0.59637 0.40363]]\n",
      "\n",
      " [[0.63257 0.36743]\n",
      "  [0.59144 0.40856]]]\n",
      "emission matrices B:\n",
      " [[[0.14936 0.14634 0.1517  0.14884 0.15103 0.25273]\n",
      "  [0.14815 0.15786 0.15223 0.15495 0.1563  0.23051]]\n",
      "\n",
      " [[0.14834 0.15201 0.146   0.15079 0.15251 0.25034]\n",
      "  [0.14909 0.14899 0.16187 0.15169 0.15368 0.23467]]]\n",
      "initial distributions I:\n",
      " [[0.1861  0.8139 ]\n",
      " [0.27128 0.72872]]\n",
      "Epoch 012: Loss: 176.877\n",
      "Epoch 014: Loss: 176.884\n",
      "Epoch 016: Loss: 176.931\n",
      "Epoch 018: Loss: 176.951\n",
      "Epoch 020: Loss: 177.115\n",
      "transition matrices A:\n",
      " [[[0.70658 0.29342]\n",
      "  [0.49668 0.50332]]\n",
      "\n",
      " [[0.69381 0.30619]\n",
      "  [0.52118 0.47882]]]\n",
      "emission matrices B:\n",
      " [[[0.14725 0.14956 0.15059 0.14571 0.14435 0.26253]\n",
      "  [0.154   0.1593  0.15637 0.15713 0.15659 0.2166 ]]\n",
      "\n",
      " [[0.14736 0.15438 0.14596 0.14771 0.14592 0.25867]\n",
      "  [0.15357 0.15151 0.16364 0.15391 0.15418 0.22319]]]\n",
      "initial distributions I:\n",
      " [[0.07526 0.92474]\n",
      " [0.11072 0.88928]]\n",
      "Epoch 022: Loss: 177.446\n",
      "Epoch 024: Loss: 176.963\n",
      "Epoch 026: Loss: 176.878\n",
      "Epoch 028: Loss: 177.071\n",
      "Epoch 030: Loss: 176.732\n",
      "transition matrices A:\n",
      " [[[0.89997 0.10003]\n",
      "  [0.1162  0.8838 ]]\n",
      "\n",
      " [[0.87253 0.12747]\n",
      "  [0.17554 0.82446]]]\n",
      "emission matrices B:\n",
      " [[[0.12844 0.12854 0.13049 0.13585 0.12915 0.34752]\n",
      "  [0.16804 0.17208 0.16591 0.17132 0.1717  0.15095]]\n",
      "\n",
      " [[0.13561 0.13844 0.133   0.14361 0.13578 0.31356]\n",
      "  [0.16292 0.16555 0.1664  0.16559 0.16803 0.17151]]]\n",
      "initial distributions I:\n",
      " [[0.02302 0.97698]\n",
      " [0.0305  0.9695 ]]\n",
      "Epoch 032: Loss: 176.427\n",
      "Epoch 034: Loss: 176.268\n",
      "Epoch 036: Loss: 176.227\n",
      "Epoch 038: Loss: 176.325\n",
      "Epoch 040: Loss: 176.329\n",
      "transition matrices A:\n",
      " [[[0.87524 0.12476]\n",
      "  [0.06454 0.93546]]\n",
      "\n",
      " [[0.87832 0.12168]\n",
      "  [0.06558 0.93442]]]\n",
      "emission matrices B:\n",
      " [[[0.10853 0.10944 0.11404 0.1136  0.11018 0.44421]\n",
      "  [0.166   0.1722  0.17183 0.17223 0.17046 0.14727]]\n",
      "\n",
      " [[0.10989 0.11095 0.11536 0.11483 0.11147 0.4375 ]\n",
      "  [0.16642 0.17244 0.17205 0.17244 0.17083 0.14582]]]\n",
      "initial distributions I:\n",
      " [[0.04397 0.95603]\n",
      " [0.0462  0.9538 ]]\n"
     ]
    }
   ],
   "source": [
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "\n",
    "num_epochs = 41\n",
    "m = 10 # training batches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "  # Training loop - using batches\n",
    "  for y in ds.take(m):\n",
    "    # Optimize the model\n",
    "    loss_value, grads = grad(model, y)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "\n",
    "    # End epoch\n",
    "  train_loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "  if epoch % 2 == 0:\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "  if epoch % 10 == 0:\n",
    "    print_pars(dcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrices A:\n",
      " [[[0.87524 0.12476]\n",
      "  [0.06454 0.93546]]\n",
      "\n",
      " [[0.87832 0.12168]\n",
      "  [0.06558 0.93442]]]\n",
      "emission matrices B:\n",
      " [[[0.10853 0.10944 0.11404 0.1136  0.11018 0.44421]\n",
      "  [0.166   0.1722  0.17183 0.17223 0.17046 0.14727]]\n",
      "\n",
      " [[0.10989 0.11095 0.11536 0.11483 0.11147 0.4375 ]\n",
      "  [0.16642 0.17244 0.17205 0.17244 0.17083 0.14582]]]\n",
      "initial distributions I:\n",
      " [[0.04397 0.95603]\n",
      " [0.0462  0.9538 ]]\n"
     ]
    }
   ],
   "source": [
    "print_pars(dcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
